{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  <center>Mercedes Benz Greener Manufacturing \n",
    "<center> Johannes Rist </center>\n",
    "    \n",
    "<center> Version 1, 20.09.2020 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand dieses prototypenhaften Aufbaus wird die Implementierung eines neuronalen Netzes für eine Problemstellung aus der Produktionsumgebung dargelegt. \n",
    "Viele Firmen (darunter auch die MTU Friedrichshafen) erheben mittlerweile viele Daten in der Produktion. Ziel der Datenaufnahme ist dabei häufig die Dokumentation des Produktionsprozesses. Diese Datensätze weisen zwar viele Produktionsparameter auf, jedoch fehlt meist ein Bezug zu einer Zielvariablen, sodass die Voraussetzung für das Training eines neuronalen Netzes nicht gegeben ist. Aufgrund dessen sind die meisten Datensätze für Data-Science Zwecke ungeeignet. Für das folgende Beispiel wird deshalb ein frei zugänglicher anonymisierter Datensatz aus dem Internet verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wird Keras, das Framework zur Modellierung neuronaler Netze und Pandas, eine Programmbibliothek zur Datenverwaltung eingelesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend wird der Datensatz importiert und angezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>130.81</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>88.53</td>\n",
       "      <td>k</td>\n",
       "      <td>t</td>\n",
       "      <td>av</td>\n",
       "      <td>e</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>76.26</td>\n",
       "      <td>az</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>j</td>\n",
       "      <td>x</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>80.62</td>\n",
       "      <td>az</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>78.02</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>8405</td>\n",
       "      <td>107.39</td>\n",
       "      <td>ak</td>\n",
       "      <td>s</td>\n",
       "      <td>as</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>d</td>\n",
       "      <td>q</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>8406</td>\n",
       "      <td>108.77</td>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>t</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>8412</td>\n",
       "      <td>109.22</td>\n",
       "      <td>ak</td>\n",
       "      <td>v</td>\n",
       "      <td>r</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>8415</td>\n",
       "      <td>87.48</td>\n",
       "      <td>al</td>\n",
       "      <td>r</td>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>l</td>\n",
       "      <td>u</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>8417</td>\n",
       "      <td>110.85</td>\n",
       "      <td>z</td>\n",
       "      <td>r</td>\n",
       "      <td>ae</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID       y  X0 X1  X2 X3 X4  X5 X6 X8  ...  X375  X376  X377  X378  \\\n",
       "0        0  130.81   k  v  at  a  d   u  j  o  ...     0     0     1     0   \n",
       "1        6   88.53   k  t  av  e  d   y  l  o  ...     1     0     0     0   \n",
       "2        7   76.26  az  w   n  c  d   x  j  x  ...     0     0     0     0   \n",
       "3        9   80.62  az  t   n  f  d   x  l  e  ...     0     0     0     0   \n",
       "4       13   78.02  az  v   n  f  d   h  d  n  ...     0     0     0     0   \n",
       "...    ...     ...  .. ..  .. .. ..  .. .. ..  ...   ...   ...   ...   ...   \n",
       "4204  8405  107.39  ak  s  as  c  d  aa  d  q  ...     1     0     0     0   \n",
       "4205  8406  108.77   j  o   t  d  d  aa  h  h  ...     0     1     0     0   \n",
       "4206  8412  109.22  ak  v   r  a  d  aa  g  e  ...     0     0     1     0   \n",
       "4207  8415   87.48  al  r   e  f  d  aa  l  u  ...     0     0     0     0   \n",
       "4208  8417  110.85   z  r  ae  c  d  aa  g  w  ...     1     0     0     0   \n",
       "\n",
       "      X379  X380  X382  X383  X384  X385  \n",
       "0        0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0  \n",
       "2        0     0     1     0     0     0  \n",
       "3        0     0     0     0     0     0  \n",
       "4        0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...  \n",
       "4204     0     0     0     0     0     0  \n",
       "4205     0     0     0     0     0     0  \n",
       "4206     0     0     0     0     0     0  \n",
       "4207     0     0     0     0     0     0  \n",
       "4208     0     0     0     0     0     0  \n",
       "\n",
       "[4209 rows x 378 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataPath=r\"C:\\Users\\johan\\Desktop\\MercedesBenzGreenerManufacturing\\data.csv\"\n",
    "data = pd.read_csv(DataPath, delimiter=',')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Geschäftsprozessverständnis\n",
    "Der Mercedes-Benz-Greener-Manufacturing-Datensatz enthält anonymisierte Produktionsdaten der Firma Mercedes Benz und entstammt der Website Kaggle.io, einer Plattform für Data Science Competitions. Dort werden die Teilnehmer dazu aufgefordert, anhand von Permutationen von Mercedes-Benz-Fahrzeugmerkmalen vorauszusagen, wie lange das jeweilige Fahrzeug auf dem Prüfstand benötigen wird, um die Prüfung zu bestehen. Die von den Teilnehmern erzeugten Modelle sollen zu einer schnelleren Prüfung beitragen und somit helfen geringere Kohlenstoffdioxidemissionen zu erzeugen, ohne die Standards von Mercedes Benz zu reduzieren. Da in der Beschreibung des Use Cases auf Kaggle nicht angegeben ist, anhand welcher Kriterien Mercedes Benz derzeit die geplante Prüfdauer pro Fahrzeug festlegt, wird für dieses Beispiel davon ausgegangen, dass hierfür der Mittelwert aller Prüfzeiten verwendet wird. Ziel der Implementierung des neuronalen Netzes ist es also, Voraussagen anhand der Fahrzeugmerkmale zu treffen, die genauer sind als die einfache Anwendung des Mittelwerts aller Prüfzeiten. Als Metrik soll der Mean Absolute Error (MAE) verwendet werden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Berechnung des MAE unter Verwendung des Mittelwerts über alle Prüfzeiten, wird zunächst die Prüfzeit aus dem Datensatz extrahiert und im Dataframe Y abgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>107.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>108.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>109.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>87.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>110.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           y\n",
       "0     130.81\n",
       "1      88.53\n",
       "2      76.26\n",
       "3      80.62\n",
       "4      78.02\n",
       "...      ...\n",
       "4204  107.39\n",
       "4205  108.77\n",
       "4206  109.22\n",
       "4207   87.48\n",
       "4208  110.85\n",
       "\n",
       "[4209 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=pd.DataFrame(data.y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend wird der Mittelwert aller Prüfzeiten berechnet und der Variablen y_mean zugewiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y    100.669318\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mean=y.mean()\n",
    "y_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danach wird der Mittelwert von jedem Wert im Dataframe y subtrahiert und im Dataframe y_err abgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.140682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.139318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-24.409318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-20.049318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-22.649318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>6.720682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>8.100682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>8.550682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>-13.189318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>10.180682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              y\n",
       "0     30.140682\n",
       "1    -12.139318\n",
       "2    -24.409318\n",
       "3    -20.049318\n",
       "4    -22.649318\n",
       "...         ...\n",
       "4204   6.720682\n",
       "4205   8.100682\n",
       "4206   8.550682\n",
       "4207 -13.189318\n",
       "4208  10.180682\n",
       "\n",
       "[4209 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_err=y-y_mean\n",
    "y_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anwendung der Funktion abs() auf das Dataframe y_err bildet von allen Werten den Absolutwert und legt diesen im Dataframe der Variable y_err_abs ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.140682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.139318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.409318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.049318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.649318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>6.720682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>8.100682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>8.550682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>13.189318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>10.180682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              y\n",
       "0     30.140682\n",
       "1     12.139318\n",
       "2     24.409318\n",
       "3     20.049318\n",
       "4     22.649318\n",
       "...         ...\n",
       "4204   6.720682\n",
       "4205   8.100682\n",
       "4206   8.550682\n",
       "4207  13.189318\n",
       "4208  10.180682\n",
       "\n",
       "[4209 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_err_abs=abs(y_err)\n",
    "y_err_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch Anwenden der mean()-Funktion auf das Dataframe y_err_abs wird der initiale Mean Absolute Error berechnet, für den Fall, dass für jedes Fahrzeug der Mittelwert als geplante Prüfzeit angenommen wird. Dieser MAE wird der Variablen MAE_initial zugewiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y    10.088697\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE_initial=y_err_abs.mean()\n",
    "MAE_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Datenverständnis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prüfung auf NA- und NULL-Werte\n",
    "Die Funktion isnull() zählt NA/NULL-Werte und gibt für jeden Wert ein True oder False aus. Durch anschließende Anwendung der Funktion sum() werden diese für jede Zeile aufsummiert und in die Variable NaN geschrieben. Ein weiteres Anwenden der Funktion sum() auf die Variable NaN summiert die Werte der darin enthaltenen Liste auf. Enthält der Datensatz keine NA- oder NULL-Werte muss das Ergebnis 0 lauten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID      0\n",
       "y       0\n",
       "X0      0\n",
       "X1      0\n",
       "X2      0\n",
       "       ..\n",
       "X380    0\n",
       "X382    0\n",
       "X383    0\n",
       "X384    0\n",
       "X385    0\n",
       "Length: 378, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaN=data.isnull().sum()\n",
    "NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaN.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datentypen\n",
    "Vor der Betrachtung der Datentypen wird zunächst die Spalte y, die die Zielwerte enthält, in die Variable labels kopiert. Diese wird anschließend gemeinsam mit der Spalte 'ID', von der keine Informationen bezüglich der Prüfdauer zu erwarten sind, aus dem DataFrame data entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k</td>\n",
       "      <td>t</td>\n",
       "      <td>av</td>\n",
       "      <td>e</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>az</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>j</td>\n",
       "      <td>x</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>az</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>ak</td>\n",
       "      <td>s</td>\n",
       "      <td>as</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>d</td>\n",
       "      <td>q</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>t</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>ak</td>\n",
       "      <td>v</td>\n",
       "      <td>r</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>al</td>\n",
       "      <td>r</td>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>l</td>\n",
       "      <td>u</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>z</td>\n",
       "      <td>r</td>\n",
       "      <td>ae</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 376 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X0 X1  X2 X3 X4  X5 X6 X8  X10  X11  ...  X375  X376  X377  X378  X379  \\\n",
       "0      k  v  at  a  d   u  j  o    0    0  ...     0     0     1     0     0   \n",
       "1      k  t  av  e  d   y  l  o    0    0  ...     1     0     0     0     0   \n",
       "2     az  w   n  c  d   x  j  x    0    0  ...     0     0     0     0     0   \n",
       "3     az  t   n  f  d   x  l  e    0    0  ...     0     0     0     0     0   \n",
       "4     az  v   n  f  d   h  d  n    0    0  ...     0     0     0     0     0   \n",
       "...   .. ..  .. .. ..  .. .. ..  ...  ...  ...   ...   ...   ...   ...   ...   \n",
       "4204  ak  s  as  c  d  aa  d  q    0    0  ...     1     0     0     0     0   \n",
       "4205   j  o   t  d  d  aa  h  h    0    0  ...     0     1     0     0     0   \n",
       "4206  ak  v   r  a  d  aa  g  e    0    0  ...     0     0     1     0     0   \n",
       "4207  al  r   e  f  d  aa  l  u    0    0  ...     0     0     0     0     0   \n",
       "4208   z  r  ae  c  d  aa  g  w    0    0  ...     1     0     0     0     0   \n",
       "\n",
       "      X380  X382  X383  X384  X385  \n",
       "0        0     0     0     0     0  \n",
       "1        0     0     0     0     0  \n",
       "2        0     1     0     0     0  \n",
       "3        0     0     0     0     0  \n",
       "4        0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...  \n",
       "4204     0     0     0     0     0  \n",
       "4205     0     0     0     0     0  \n",
       "4206     0     0     0     0     0  \n",
       "4207     0     0     0     0     0  \n",
       "4208     0     0     0     0     0  \n",
       "\n",
       "[4209 rows x 376 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=data['y']\n",
    "data=data.drop(columns=['y','ID'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes werden die Datentypen im Datensatz untersucht, um zu prüfen, welche Vorverarbeitungsschritte später notwendig sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('O'), dtype('int64')], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die weitere Untersuchung werden die Spaltenbezeichnungen numerischer und nichtnumerischer Datensätze in die Variablen numerical und notNumerical gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical=[]\n",
    "notNumerical=[]\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'int64' or data[col].dtype == 'float64':    \n",
    "        numerical.append(col)\n",
    "    if data[col].dtype == 'O':   \n",
    "        notNumerical.append(col)\n",
    "len(numerical),len(notNumerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerische Daten\n",
    "Zunächst erfolgt die Betrachtung der numerischen Daten. Auf den ersten Blick lassen sich darin nur binäre Daten vermuten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 368 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X10  X11  X12  X13  X14  X15  X16  X17  X18  X19  ...  X375  X376  X377  \\\n",
       "0       0    0    0    1    0    0    0    0    1    0  ...     0     0     1   \n",
       "1       0    0    0    0    0    0    0    0    1    0  ...     1     0     0   \n",
       "2       0    0    0    0    0    0    0    1    0    0  ...     0     0     0   \n",
       "3       0    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "4       0    0    0    0    0    0    0    0    0    0  ...     0     0     0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   \n",
       "4204    0    0    0    0    1    0    0    0    0    0  ...     1     0     0   \n",
       "4205    0    0    0    0    0    0    0    0    0    0  ...     0     1     0   \n",
       "4206    0    0    1    1    0    0    0    0    0    0  ...     0     0     1   \n",
       "4207    0    0    0    0    1    0    0    0    0    0  ...     0     0     0   \n",
       "4208    0    0    0    0    0    0    0    0    0    0  ...     1     0     0   \n",
       "\n",
       "      X378  X379  X380  X382  X383  X384  X385  \n",
       "0        0     0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0     0  \n",
       "2        0     0     0     1     0     0     0  \n",
       "3        0     0     0     0     0     0     0  \n",
       "4        0     0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...  \n",
       "4204     0     0     0     0     0     0     0  \n",
       "4205     0     0     0     0     0     0     0  \n",
       "4206     0     0     0     0     0     0     0  \n",
       "4207     0     0     0     0     0     0     0  \n",
       "4208     0     0     0     0     0     0     0  \n",
       "\n",
       "[4209 rows x 368 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[numerical]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Bestätigung dieser Vermutung wird eine Schleife verwendet, die die Spaltennamen aller Spalten, welche die Voraussetzung für Binarität erfüllen, in die Liste binary und alle Übrigen in die Liste notBinary speichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary=[]\n",
    "notBinary=[]\n",
    "for col in data[numerical].columns:\n",
    "    if data[col].min() == 0 and data[col].max() == 1 and data[col].dtype == 'int64':    \n",
    "        binary.append(col)\n",
    "    else:   \n",
    "        notBinary.append(col)\n",
    "len(binary),len(notBinary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die binären Attribute ist später keine weitere Vorverarbeitung notwendig. Diese erfüllen die Voraussetzungen für die Anwendung neuronaler Netze. Die Betrachtung der nichtbinären Daten lässt vermuten, dass diese nur Nullwerte enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X11</th>\n",
       "      <th>X93</th>\n",
       "      <th>X107</th>\n",
       "      <th>X233</th>\n",
       "      <th>X235</th>\n",
       "      <th>X268</th>\n",
       "      <th>X289</th>\n",
       "      <th>X290</th>\n",
       "      <th>X293</th>\n",
       "      <th>X297</th>\n",
       "      <th>X330</th>\n",
       "      <th>X347</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X11  X93  X107  X233  X235  X268  X289  X290  X293  X297  X330  X347\n",
       "0       0    0     0     0     0     0     0     0     0     0     0     0\n",
       "1       0    0     0     0     0     0     0     0     0     0     0     0\n",
       "2       0    0     0     0     0     0     0     0     0     0     0     0\n",
       "3       0    0     0     0     0     0     0     0     0     0     0     0\n",
       "4       0    0     0     0     0     0     0     0     0     0     0     0\n",
       "...   ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "4204    0    0     0     0     0     0     0     0     0     0     0     0\n",
       "4205    0    0     0     0     0     0     0     0     0     0     0     0\n",
       "4206    0    0     0     0     0     0     0     0     0     0     0     0\n",
       "4207    0    0     0     0     0     0     0     0     0     0     0     0\n",
       "4208    0    0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "[4209 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[notBinary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch Bilden des Maximums und Minimums jeder Spalte und der anschließenden Aufsummierung wird diese Vermutung bestätigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[notBinary].max().sum(),data[notBinary].max().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nichtnumerische Daten\n",
    "Als nächstes werden die nichtnumerischen Attribute weiter untersucht, welche mutmaßlich kategorische Daten enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4209</td>\n",
       "      <td>4209</td>\n",
       "      <td>4209</td>\n",
       "      <td>4209</td>\n",
       "      <td>4209</td>\n",
       "      <td>4209</td>\n",
       "      <td>4209</td>\n",
       "      <td>4209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>47</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>z</td>\n",
       "      <td>aa</td>\n",
       "      <td>as</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "      <td>g</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>360</td>\n",
       "      <td>833</td>\n",
       "      <td>1659</td>\n",
       "      <td>1942</td>\n",
       "      <td>4205</td>\n",
       "      <td>231</td>\n",
       "      <td>1042</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X0    X1    X2    X3    X4    X5    X6    X8\n",
       "count   4209  4209  4209  4209  4209  4209  4209  4209\n",
       "unique    47    27    44     7     4    29    12    25\n",
       "top        z    aa    as     c     d     w     g     j\n",
       "freq     360   833  1659  1942  4205   231  1042   277"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[notNumerical].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies kann allerdings aufgrund der Anonymisierung des Datensatzes nicht weiter überprüft werden. Ebenfalls lässt sich daraus nicht schließen, ob die Daten eine ordinale oder nominale Struktur haben. Deshalb wird für das weitere Vorgehen die Annahme getroffen, bei den Spalten X0-X8 handle es sich um kategorisch ordinale Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Datenvorbereitung\n",
    "\n",
    "In dieser Phase werden die Daten für die Anwendung neuronaler Netze vorbereitet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerische Daten\n",
    "Die in Kapitel 2 als nicht-binäre numerische Attribute klassifizierten Daten enthalten keinerlei Information. Da der Bedarf an Samples mit steigender Anzahl an Attributen ebenfalls wächst, empfiehlt es sich diese Attribute zu löschen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>X10</th>\n",
       "      <th>X12</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k</td>\n",
       "      <td>t</td>\n",
       "      <td>av</td>\n",
       "      <td>e</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>az</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>j</td>\n",
       "      <td>x</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>az</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>ak</td>\n",
       "      <td>s</td>\n",
       "      <td>as</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>d</td>\n",
       "      <td>q</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>t</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>ak</td>\n",
       "      <td>v</td>\n",
       "      <td>r</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>al</td>\n",
       "      <td>r</td>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>l</td>\n",
       "      <td>u</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>z</td>\n",
       "      <td>r</td>\n",
       "      <td>ae</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>aa</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X0 X1  X2 X3 X4  X5 X6 X8  X10  X12  ...  X375  X376  X377  X378  X379  \\\n",
       "0      k  v  at  a  d   u  j  o    0    0  ...     0     0     1     0     0   \n",
       "1      k  t  av  e  d   y  l  o    0    0  ...     1     0     0     0     0   \n",
       "2     az  w   n  c  d   x  j  x    0    0  ...     0     0     0     0     0   \n",
       "3     az  t   n  f  d   x  l  e    0    0  ...     0     0     0     0     0   \n",
       "4     az  v   n  f  d   h  d  n    0    0  ...     0     0     0     0     0   \n",
       "...   .. ..  .. .. ..  .. .. ..  ...  ...  ...   ...   ...   ...   ...   ...   \n",
       "4204  ak  s  as  c  d  aa  d  q    0    0  ...     1     0     0     0     0   \n",
       "4205   j  o   t  d  d  aa  h  h    0    0  ...     0     1     0     0     0   \n",
       "4206  ak  v   r  a  d  aa  g  e    0    1  ...     0     0     1     0     0   \n",
       "4207  al  r   e  f  d  aa  l  u    0    0  ...     0     0     0     0     0   \n",
       "4208   z  r  ae  c  d  aa  g  w    0    0  ...     1     0     0     0     0   \n",
       "\n",
       "      X380  X382  X383  X384  X385  \n",
       "0        0     0     0     0     0  \n",
       "1        0     0     0     0     0  \n",
       "2        0     1     0     0     0  \n",
       "3        0     0     0     0     0  \n",
       "4        0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...  \n",
       "4204     0     0     0     0     0  \n",
       "4205     0     0     0     0     0  \n",
       "4206     0     0     0     0     0  \n",
       "4207     0     0     0     0     0  \n",
       "4208     0     0     0     0     0  \n",
       "\n",
       "[4209 rows x 364 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.drop(columns=notBinary)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nichtnumerische Daten\n",
    "Diese Attribute müssen zunächst vektorisiert, das bedeutet in Zahlenwerte umgewandelt werden. Dafür kann beispielsweise die Funktion LabelEncoder aus der Bibliothek von ScikitLearn verwendet werden. In einer Schleife werden Spalte für Spalte die Werte codiert und als neue Spalten an das DataFrame data angehängt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>X10</th>\n",
       "      <th>X12</th>\n",
       "      <th>...</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "      <th>X0_code</th>\n",
       "      <th>X1_code</th>\n",
       "      <th>X2_code</th>\n",
       "      <th>X3_code</th>\n",
       "      <th>X4_code</th>\n",
       "      <th>X5_code</th>\n",
       "      <th>X6_code</th>\n",
       "      <th>X8_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "      <td>j</td>\n",
       "      <td>o</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k</td>\n",
       "      <td>t</td>\n",
       "      <td>av</td>\n",
       "      <td>e</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>l</td>\n",
       "      <td>o</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>az</td>\n",
       "      <td>w</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>j</td>\n",
       "      <td>x</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>az</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 372 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X0 X1  X2 X3 X4 X5 X6 X8  X10  X12  ...  X384  X385  X0_code  X1_code  \\\n",
       "0   k  v  at  a  d  u  j  o    0    0  ...     0     0       32       23   \n",
       "1   k  t  av  e  d  y  l  o    0    0  ...     0     0       32       21   \n",
       "2  az  w   n  c  d  x  j  x    0    0  ...     0     0       20       24   \n",
       "3  az  t   n  f  d  x  l  e    0    0  ...     0     0       20       21   \n",
       "4  az  v   n  f  d  h  d  n    0    0  ...     0     0       20       23   \n",
       "\n",
       "   X2_code  X3_code  X4_code  X5_code  X6_code  X8_code  \n",
       "0       17        0        3       24        9       14  \n",
       "1       19        4        3       28       11       14  \n",
       "2       34        2        3       27        9       23  \n",
       "3       34        5        3       27       11        4  \n",
       "4       34        5        3       12        3       13  \n",
       "\n",
       "[5 rows x 372 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "for col in data[notNumerical].columns:  \n",
    "     data[col+\"_code\"] = lb_make.fit_transform(data[col])\n",
    "data.head()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die ursprünglichen Spalten müssen anschließend entfernt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=notNumerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da die Einzelwerte der codierten Attribute noch nicht in einem Wertebereich nahe 0 liegen, müssen diese nun noch normiert werden. Dazu wird zunächst in einer Schleife die zuvor deklarierte Liste notNumerical_code mit den Spaltennamen der codierten Attribute befüllt. Anschließend wird, ebenfalls in einer Schleife, von jedem Wert in der Spalte zunächst der Spalten-Mittelwert subtrahiert, wodurch die Werte um den neuen Mittelwert 0 streuen. Im nächsten Schritt werden alle Werte durch die Standardabweichung geteilt. Dadurch befinden sich anschließend alle Werte in einem Wertebereich nahe der 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X10</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>...</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "      <th>X0_code</th>\n",
       "      <th>X1_code</th>\n",
       "      <th>X2_code</th>\n",
       "      <th>X3_code</th>\n",
       "      <th>X4_code</th>\n",
       "      <th>X5_code</th>\n",
       "      <th>X6_code</th>\n",
       "      <th>X8_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.162993</td>\n",
       "      <td>1.393322</td>\n",
       "      <td>-0.028118</td>\n",
       "      <td>-1.678071</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.291964</td>\n",
       "      <td>0.751698</td>\n",
       "      <td>0.339405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.162993</td>\n",
       "      <td>1.158883</td>\n",
       "      <td>0.155369</td>\n",
       "      <td>0.620896</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.776763</td>\n",
       "      <td>1.437340</td>\n",
       "      <td>0.339405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.710475</td>\n",
       "      <td>1.510542</td>\n",
       "      <td>1.531527</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.655563</td>\n",
       "      <td>0.751698</td>\n",
       "      <td>1.618197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.710475</td>\n",
       "      <td>1.158883</td>\n",
       "      <td>1.531527</td>\n",
       "      <td>1.195637</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.655563</td>\n",
       "      <td>1.437340</td>\n",
       "      <td>-1.081476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.710475</td>\n",
       "      <td>1.393322</td>\n",
       "      <td>1.531527</td>\n",
       "      <td>1.195637</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-0.162435</td>\n",
       "      <td>-1.305229</td>\n",
       "      <td>0.197316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.583943</td>\n",
       "      <td>1.041664</td>\n",
       "      <td>-0.119862</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>-1.305229</td>\n",
       "      <td>0.623581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090204</td>\n",
       "      <td>0.572786</td>\n",
       "      <td>2.081990</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>0.066056</td>\n",
       "      <td>-0.655212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.583943</td>\n",
       "      <td>1.393322</td>\n",
       "      <td>1.898502</td>\n",
       "      <td>-1.678071</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>-0.276766</td>\n",
       "      <td>-1.081476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.511154</td>\n",
       "      <td>0.924444</td>\n",
       "      <td>0.705833</td>\n",
       "      <td>1.195637</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>1.437340</td>\n",
       "      <td>1.191933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.182039</td>\n",
       "      <td>0.924444</td>\n",
       "      <td>-1.312532</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>-0.276766</td>\n",
       "      <td>1.476109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows × 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X10  X12  X13  X14  X15  X16  X17  X18  X19  X20  ...  X384  X385  \\\n",
       "0       0    0    1    0    0    0    0    1    0    0  ...     0     0   \n",
       "1       0    0    0    0    0    0    0    1    0    0  ...     0     0   \n",
       "2       0    0    0    0    0    0    1    0    0    0  ...     0     0   \n",
       "3       0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "4       0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "4204    0    0    0    1    0    0    0    0    0    0  ...     0     0   \n",
       "4205    0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "4206    0    1    1    0    0    0    0    0    0    0  ...     0     0   \n",
       "4207    0    0    0    1    0    0    0    0    0    0  ...     0     0   \n",
       "4208    0    0    0    0    0    0    0    0    0    1  ...     0     0   \n",
       "\n",
       "       X0_code   X1_code   X2_code   X3_code   X4_code   X5_code   X6_code  \\\n",
       "0     0.162993  1.393322 -0.028118 -1.678071  0.028935  1.291964  0.751698   \n",
       "1     0.162993  1.158883  0.155369  0.620896  0.028935  1.776763  1.437340   \n",
       "2    -0.710475  1.510542  1.531527 -0.528587  0.028935  1.655563  0.751698   \n",
       "3    -0.710475  1.158883  1.531527  1.195637  0.028935  1.655563  1.437340   \n",
       "4    -0.710475  1.393322  1.531527  1.195637  0.028935 -0.162435 -1.305229   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4204 -1.583943  1.041664 -0.119862 -0.528587  0.028935 -1.616834 -1.305229   \n",
       "4205  0.090204  0.572786  2.081990  0.046154  0.028935 -1.616834  0.066056   \n",
       "4206 -1.583943  1.393322  1.898502 -1.678071  0.028935 -1.616834 -0.276766   \n",
       "4207 -1.511154  0.924444  0.705833  1.195637  0.028935 -1.616834  1.437340   \n",
       "4208  1.182039  0.924444 -1.312532 -0.528587  0.028935 -1.616834 -0.276766   \n",
       "\n",
       "       X8_code  \n",
       "0     0.339405  \n",
       "1     0.339405  \n",
       "2     1.618197  \n",
       "3    -1.081476  \n",
       "4     0.197316  \n",
       "...        ...  \n",
       "4204  0.623581  \n",
       "4205 -0.655212  \n",
       "4206 -1.081476  \n",
       "4207  1.191933  \n",
       "4208  1.476109  \n",
       "\n",
       "[4209 rows x 364 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notNumerical_code=[]\n",
    "for x in notNumerical:\n",
    "    x=x+'_code'\n",
    "    notNumerical_code.append(x)\n",
    "notNumerical_code\n",
    "\n",
    "for col in data[notNumerical_code].columns: \n",
    "    data[col]=data[col] - data[col].mean(axis=0)\n",
    "    data[col]=data[col] / data[col].std(axis=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Aufteilung\n",
    "Als letzter Schritt der Datenvorbereitung erfolgt die Train-Test-Aufteilung. Für den Test des Modells werden dafür 20% der Daten zurückgehalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_percentage=0.2\n",
    "num_test_samples=int(test_percentage*len(data))\n",
    "num_test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Test-Samples werden nun die 841 letzten Samples des Datensatzes data in die Variable test_data geschrieben. Die übrigen Samples werden der Variablen train_data zugeteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X10</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>...</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "      <th>X0_code</th>\n",
       "      <th>X1_code</th>\n",
       "      <th>X2_code</th>\n",
       "      <th>X3_code</th>\n",
       "      <th>X4_code</th>\n",
       "      <th>X5_code</th>\n",
       "      <th>X6_code</th>\n",
       "      <th>X8_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.109250</td>\n",
       "      <td>0.572786</td>\n",
       "      <td>0.797576</td>\n",
       "      <td>1.195637</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>1.437340</td>\n",
       "      <td>1.618197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.036461</td>\n",
       "      <td>-0.130532</td>\n",
       "      <td>-0.119862</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>0.751698</td>\n",
       "      <td>0.765669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.200952</td>\n",
       "      <td>-0.833849</td>\n",
       "      <td>1.439783</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>0.751698</td>\n",
       "      <td>-1.081476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3371</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090204</td>\n",
       "      <td>-0.130532</td>\n",
       "      <td>-0.119862</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>1.437340</td>\n",
       "      <td>0.197316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.381360</td>\n",
       "      <td>-1.302727</td>\n",
       "      <td>-0.853813</td>\n",
       "      <td>1.195637</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>-1.305229</td>\n",
       "      <td>-0.086860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.583943</td>\n",
       "      <td>1.041664</td>\n",
       "      <td>-0.119862</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>-1.305229</td>\n",
       "      <td>0.623581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090204</td>\n",
       "      <td>0.572786</td>\n",
       "      <td>2.081990</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>0.066056</td>\n",
       "      <td>-0.655212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.583943</td>\n",
       "      <td>1.393322</td>\n",
       "      <td>1.898502</td>\n",
       "      <td>-1.678071</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>-0.276766</td>\n",
       "      <td>-1.081476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.511154</td>\n",
       "      <td>0.924444</td>\n",
       "      <td>0.705833</td>\n",
       "      <td>1.195637</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>1.437340</td>\n",
       "      <td>1.191933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.182039</td>\n",
       "      <td>0.924444</td>\n",
       "      <td>-1.312532</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-1.616834</td>\n",
       "      <td>-0.276766</td>\n",
       "      <td>1.476109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>841 rows × 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X10  X12  X13  X14  X15  X16  X17  X18  X19  X20  ...  X384  X385  \\\n",
       "3368    0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "3369    0    0    0    1    0    0    0    0    0    0  ...     0     0   \n",
       "3370    0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "3371    0    0    0    1    0    0    0    0    0    0  ...     0     0   \n",
       "3372    0    0    0    0    0    0    0    0    0    1  ...     0     0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "4204    0    0    0    1    0    0    0    0    0    0  ...     0     0   \n",
       "4205    0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "4206    0    1    1    0    0    0    0    0    0    0  ...     0     0   \n",
       "4207    0    0    0    1    0    0    0    0    0    0  ...     0     0   \n",
       "4208    0    0    0    0    0    0    0    0    0    1  ...     0     0   \n",
       "\n",
       "       X0_code   X1_code   X2_code   X3_code   X4_code   X5_code   X6_code  \\\n",
       "3368  1.109250  0.572786  0.797576  1.195637  0.028935  1.170764  1.437340   \n",
       "3369  1.036461 -0.130532 -0.119862 -0.528587  0.028935  1.170764  0.751698   \n",
       "3370 -0.200952 -0.833849  1.439783 -0.528587  0.028935  1.170764  0.751698   \n",
       "3371  0.090204 -0.130532 -0.119862 -0.528587  0.028935  1.170764  1.437340   \n",
       "3372  0.381360 -1.302727 -0.853813  1.195637  0.028935  1.170764 -1.305229   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4204 -1.583943  1.041664 -0.119862 -0.528587  0.028935 -1.616834 -1.305229   \n",
       "4205  0.090204  0.572786  2.081990  0.046154  0.028935 -1.616834  0.066056   \n",
       "4206 -1.583943  1.393322  1.898502 -1.678071  0.028935 -1.616834 -0.276766   \n",
       "4207 -1.511154  0.924444  0.705833  1.195637  0.028935 -1.616834  1.437340   \n",
       "4208  1.182039  0.924444 -1.312532 -0.528587  0.028935 -1.616834 -0.276766   \n",
       "\n",
       "       X8_code  \n",
       "3368  1.618197  \n",
       "3369  0.765669  \n",
       "3370 -1.081476  \n",
       "3371  0.197316  \n",
       "3372 -0.086860  \n",
       "...        ...  \n",
       "4204  0.623581  \n",
       "4205 -0.655212  \n",
       "4206 -1.081476  \n",
       "4207  1.191933  \n",
       "4208  1.476109  \n",
       "\n",
       "[841 rows x 364 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=data[len(data)-num_test_samples:]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X10</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>...</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "      <th>X0_code</th>\n",
       "      <th>X1_code</th>\n",
       "      <th>X2_code</th>\n",
       "      <th>X3_code</th>\n",
       "      <th>X4_code</th>\n",
       "      <th>X5_code</th>\n",
       "      <th>X6_code</th>\n",
       "      <th>X8_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.162993</td>\n",
       "      <td>1.393322</td>\n",
       "      <td>-0.028118</td>\n",
       "      <td>-1.678071</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.291964</td>\n",
       "      <td>0.751698</td>\n",
       "      <td>0.339405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.162993</td>\n",
       "      <td>1.158883</td>\n",
       "      <td>0.155369</td>\n",
       "      <td>0.620896</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.776763</td>\n",
       "      <td>1.437340</td>\n",
       "      <td>0.339405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.710475</td>\n",
       "      <td>1.510542</td>\n",
       "      <td>1.531527</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.655563</td>\n",
       "      <td>0.751698</td>\n",
       "      <td>1.618197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.710475</td>\n",
       "      <td>1.158883</td>\n",
       "      <td>1.531527</td>\n",
       "      <td>1.195637</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.655563</td>\n",
       "      <td>1.437340</td>\n",
       "      <td>-1.081476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.710475</td>\n",
       "      <td>1.393322</td>\n",
       "      <td>1.531527</td>\n",
       "      <td>1.195637</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>-0.162435</td>\n",
       "      <td>-1.305229</td>\n",
       "      <td>0.197316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.381360</td>\n",
       "      <td>1.393322</td>\n",
       "      <td>-1.312532</td>\n",
       "      <td>1.770379</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>0.751698</td>\n",
       "      <td>1.476109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.200952</td>\n",
       "      <td>-0.833849</td>\n",
       "      <td>1.439783</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>-0.276766</td>\n",
       "      <td>1.760285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.036461</td>\n",
       "      <td>0.924444</td>\n",
       "      <td>0.247113</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>0.751698</td>\n",
       "      <td>1.476109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.672516</td>\n",
       "      <td>-0.951069</td>\n",
       "      <td>-1.312532</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>-1.990871</td>\n",
       "      <td>-0.513124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.583943</td>\n",
       "      <td>0.690005</td>\n",
       "      <td>-0.119862</td>\n",
       "      <td>-0.528587</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>1.170764</td>\n",
       "      <td>1.437340</td>\n",
       "      <td>1.334021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3368 rows × 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X10  X12  X13  X14  X15  X16  X17  X18  X19  X20  ...  X384  X385  \\\n",
       "0       0    0    1    0    0    0    0    1    0    0  ...     0     0   \n",
       "1       0    0    0    0    0    0    0    1    0    0  ...     0     0   \n",
       "2       0    0    0    0    0    0    1    0    0    0  ...     0     0   \n",
       "3       0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "4       0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "3363    0    0    0    0    0    0    0    0    0    1  ...     0     0   \n",
       "3364    0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "3365    0    0    0    0    0    0    0    0    0    0  ...     0     0   \n",
       "3366    0    0    0    0    0    0    0    0    0    1  ...     0     0   \n",
       "3367    0    0    0    1    0    0    0    0    0    0  ...     0     0   \n",
       "\n",
       "       X0_code   X1_code   X2_code   X3_code   X4_code   X5_code   X6_code  \\\n",
       "0     0.162993  1.393322 -0.028118 -1.678071  0.028935  1.291964  0.751698   \n",
       "1     0.162993  1.158883  0.155369  0.620896  0.028935  1.776763  1.437340   \n",
       "2    -0.710475  1.510542  1.531527 -0.528587  0.028935  1.655563  0.751698   \n",
       "3    -0.710475  1.158883  1.531527  1.195637  0.028935  1.655563  1.437340   \n",
       "4    -0.710475  1.393322  1.531527  1.195637  0.028935 -0.162435 -1.305229   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3363  0.381360  1.393322 -1.312532  1.770379  0.028935  1.170764  0.751698   \n",
       "3364 -0.200952 -0.833849  1.439783 -0.528587  0.028935  1.170764 -0.276766   \n",
       "3365  1.036461  0.924444  0.247113  0.046154  0.028935  1.170764  0.751698   \n",
       "3366  0.672516 -0.951069 -1.312532 -0.528587  0.028935  1.170764 -1.990871   \n",
       "3367 -1.583943  0.690005 -0.119862 -0.528587  0.028935  1.170764  1.437340   \n",
       "\n",
       "       X8_code  \n",
       "0     0.339405  \n",
       "1     0.339405  \n",
       "2     1.618197  \n",
       "3    -1.081476  \n",
       "4     0.197316  \n",
       "...        ...  \n",
       "3363  1.476109  \n",
       "3364  1.760285  \n",
       "3365  1.476109  \n",
       "3366 -0.513124  \n",
       "3367  1.334021  \n",
       "\n",
       "[3368 rows x 364 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=data[:len(data)-num_test_samples]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gleiches wird auch für die Werte der Zielvariablen durchgeführt, welche in die Variablen test_labels und train_labels geschrieben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3368    105.82\n",
       "3369    108.19\n",
       "3370     94.62\n",
       "3371    110.46\n",
       "3372     88.98\n",
       "         ...  \n",
       "4204    107.39\n",
       "4205    108.77\n",
       "4206    109.22\n",
       "4207     87.48\n",
       "4208    110.85\n",
       "Name: y, Length: 841, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels=labels[len(labels)-num_test_samples:]\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       130.81\n",
       "1        88.53\n",
       "2        76.26\n",
       "3        80.62\n",
       "4        78.02\n",
       "         ...  \n",
       "3363     96.10\n",
       "3364     91.46\n",
       "3365    108.76\n",
       "3366    118.93\n",
       "3367    108.21\n",
       "Name: y, Length: 3368, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels=labels[:len(labels)-num_test_samples]\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Trainingsdaten bestehen nun aus 3368 Beobachtungen mit je 364 Attributen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3368, 364)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modellierung\n",
    "Für die Modellbildung werden zunächst die Pakete models und layers aus der Keras-Bibliothek geladen.\n",
    "Da der Datensatz mit 4209 Beobachtungen vergleichsweise klein ist, wird für das Training die k-fache Kreuzvalidierung angewendet. Dafür ist es notwendig, das Modell mehrfach zu instanziieren, weshalb dies in die Funktion build_model() ausgelagert wird. Darin wird ein kleines sequenzielles neuronales Netz mit vier Schichten definiert. Die Eingabeschicht hat als Input-Shape die 364 Attribute des Trainingsdatensatzes. Darauf folgen zwei Dense-Layer mit 256 bzw. 128 Knoten, wobei die ReLU-Funktion diesen jeweils als Aktivierungsfunktion dient. Als Ausgabeschicht wird ein Dense-Layer mit einem Knoten hinzugefügt, welches die vorauszusagende Variable y ausgeben soll. Für diese Schicht wird bewusst keine Aktivierungsfunktion definiert, da der Wertebereich der Ausgabe nicht beschränkt werden soll.\n",
    "Beim Kompilieren des Modells wird als Optimierer Adam gewählt, welcher es erlaubt die Learningrate einzustellen. Als Verlustfunktion wird der Mean Squared Error und als Performance-Metrik der Mean Absolute Error gewählt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, activation='relu',input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mittels der Variablen k wird die Anzahl der Teilmengen für die k-Fache Kreuzvalidierung definiert. Im vorliegenden Fall wird der Datensatz also in vier Teilmengen mit je 842 Samples aufgeteilt. Das neuronale Netz soll zunächst in 200 Epochen trainiert werden, um in jedem Fall eine Überanpassung zu erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Samples: 842\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "print(\"Anzahl Samples: \" + str(num_val_samples))\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weiterhin wird die Variable all_mae_histories als Liste deklariert, in der später die Validierungs-MAEs jeder Epoche dokumentiert werden. Anschließend wird eine Schleife viermal durchlaufen. Dabei wird jedes Mal eine andere Teilmenge als Validierungsdatensatz verwendet und auf den übrigen drei Teilmengen trainiert.\n",
    "Jeder Schleifendurchlauf gliedert sich in die folgenden Schritte:\n",
    "1. Ausgabe der Nummer des Faltungsprozesses\n",
    "2. Auswahl der Validierungsdaten und Zuweisung an die Variable val_data\n",
    "3. Auswahl der Validierungslabels und Zuweisung an die Variable val_labels\n",
    "4. Auswahl der Trainingsdaten und Zuweisung an die Variable partial_train_data\n",
    "5. Auswahl der Trainingslabels und Zuweisung an die Variable partial_train_labels\n",
    "6. Bilden des Modells\n",
    "7. Trainieren des Modells unter Zuweisung der historischen Daten an die Variable history\n",
    "8. Zuweisen der Liste von MAEs des aktuellen Schleifendurchlaufs an die Variable mae_history\n",
    "9. Anhängen der Liste mae_history an die Liste in der Variable all_mae_histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 2526 samples, validate on 842 samples\n",
      "Epoch 1/200\n",
      "2526/2526 [==============================] - 1s 285us/step - loss: 4646.7041 - mae: 57.5803 - val_loss: 253.0203 - val_mae: 12.5167\n",
      "Epoch 2/200\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 199.3876 - mae: 10.2271 - val_loss: 145.6014 - val_mae: 9.1663\n",
      "Epoch 3/200\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 136.2980 - mae: 8.1844 - val_loss: 102.6601 - val_mae: 7.3777\n",
      "Epoch 4/200\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 110.6342 - mae: 7.1159 - val_loss: 83.0113 - val_mae: 6.5727\n",
      "Epoch 5/200\n",
      "2526/2526 [==============================] - 0s 161us/step - loss: 97.3663 - mae: 6.5458 - val_loss: 75.2562 - val_mae: 5.7032\n",
      "Epoch 6/200\n",
      "2526/2526 [==============================] - 0s 158us/step - loss: 91.8482 - mae: 6.1953 - val_loss: 70.6813 - val_mae: 5.6755\n",
      "Epoch 7/200\n",
      "2526/2526 [==============================] - 0s 164us/step - loss: 86.9380 - mae: 5.9896 - val_loss: 70.5469 - val_mae: 5.4158\n",
      "Epoch 8/200\n",
      "2526/2526 [==============================] - 0s 180us/step - loss: 84.5453 - mae: 5.8979 - val_loss: 68.8641 - val_mae: 5.4461\n",
      "Epoch 9/200\n",
      "2526/2526 [==============================] - 0s 155us/step - loss: 84.6347 - mae: 5.8544 - val_loss: 71.3571 - val_mae: 6.0524\n",
      "Epoch 10/200\n",
      "2526/2526 [==============================] - 0s 171us/step - loss: 81.9988 - mae: 5.7816 - val_loss: 69.2539 - val_mae: 5.4224\n",
      "Epoch 11/200\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 80.0870 - mae: 5.6503 - val_loss: 70.5703 - val_mae: 5.5523\n",
      "Epoch 12/200\n",
      "2526/2526 [==============================] - 0s 188us/step - loss: 79.7713 - mae: 5.6591 - val_loss: 69.6995 - val_mae: 5.6729\n",
      "Epoch 13/200\n",
      "2526/2526 [==============================] - 0s 153us/step - loss: 79.2120 - mae: 5.6562 - val_loss: 71.8916 - val_mae: 6.0014\n",
      "Epoch 14/200\n",
      "2526/2526 [==============================] - 1s 249us/step - loss: 78.2927 - mae: 5.6224 - val_loss: 72.4446 - val_mae: 5.6909\n",
      "Epoch 15/200\n",
      "2526/2526 [==============================] - 1s 256us/step - loss: 78.0702 - mae: 5.6306 - val_loss: 74.3369 - val_mae: 5.3667\n",
      "Epoch 16/200\n",
      "2526/2526 [==============================] - 1s 314us/step - loss: 78.2480 - mae: 5.6339 - val_loss: 71.6989 - val_mae: 5.4035\n",
      "Epoch 17/200\n",
      "2526/2526 [==============================] - 1s 247us/step - loss: 76.3907 - mae: 5.5092 - val_loss: 71.4386 - val_mae: 5.5304\n",
      "Epoch 18/200\n",
      "2526/2526 [==============================] - 1s 224us/step - loss: 75.9188 - mae: 5.4936 - val_loss: 71.8576 - val_mae: 5.6950\n",
      "Epoch 19/200\n",
      "2526/2526 [==============================] - 1s 259us/step - loss: 75.5428 - mae: 5.4592 - val_loss: 72.9494 - val_mae: 5.9032\n",
      "Epoch 20/200\n",
      "2526/2526 [==============================] - 1s 210us/step - loss: 74.9944 - mae: 5.5257 - val_loss: 72.6343 - val_mae: 5.4052\n",
      "Epoch 21/200\n",
      "2526/2526 [==============================] - 1s 215us/step - loss: 74.8874 - mae: 5.4856 - val_loss: 72.9272 - val_mae: 5.2940\n",
      "Epoch 22/200\n",
      "2526/2526 [==============================] - 1s 247us/step - loss: 75.3767 - mae: 5.5223 - val_loss: 72.0699 - val_mae: 5.4927\n",
      "Epoch 23/200\n",
      "2526/2526 [==============================] - 0s 190us/step - loss: 73.5494 - mae: 5.4313 - val_loss: 72.2691 - val_mae: 5.5575\n",
      "Epoch 24/200\n",
      "2526/2526 [==============================] - 0s 181us/step - loss: 72.5418 - mae: 5.4030 - val_loss: 73.9114 - val_mae: 5.3661\n",
      "Epoch 25/200\n",
      "2526/2526 [==============================] - 1s 202us/step - loss: 74.2113 - mae: 5.4445 - val_loss: 72.7050 - val_mae: 5.5245\n",
      "Epoch 26/200\n",
      "2526/2526 [==============================] - 0s 166us/step - loss: 73.1470 - mae: 5.4187 - val_loss: 73.9384 - val_mae: 5.5337\n",
      "Epoch 27/200\n",
      "2526/2526 [==============================] - 1s 224us/step - loss: 71.4643 - mae: 5.3044 - val_loss: 74.3350 - val_mae: 5.5701\n",
      "Epoch 28/200\n",
      "2526/2526 [==============================] - 1s 223us/step - loss: 70.8971 - mae: 5.3037 - val_loss: 74.1758 - val_mae: 5.7912\n",
      "Epoch 29/200\n",
      "2526/2526 [==============================] - 0s 176us/step - loss: 70.7727 - mae: 5.2601 - val_loss: 77.2375 - val_mae: 6.1829\n",
      "Epoch 30/200\n",
      "2526/2526 [==============================] - 0s 197us/step - loss: 70.0951 - mae: 5.3303 - val_loss: 83.0636 - val_mae: 5.6414\n",
      "Epoch 31/200\n",
      "2526/2526 [==============================] - 1s 198us/step - loss: 70.0514 - mae: 5.2707 - val_loss: 75.7088 - val_mae: 5.9821\n",
      "Epoch 32/200\n",
      "2526/2526 [==============================] - 0s 180us/step - loss: 69.0242 - mae: 5.2200 - val_loss: 76.8541 - val_mae: 5.7338\n",
      "Epoch 33/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 69.0198 - mae: 5.2490 - val_loss: 76.5228 - val_mae: 5.8111\n",
      "Epoch 34/200\n",
      "2526/2526 [==============================] - 1s 212us/step - loss: 67.9012 - mae: 5.1765 - val_loss: 74.9844 - val_mae: 5.7628\n",
      "Epoch 35/200\n",
      "2526/2526 [==============================] - 1s 250us/step - loss: 66.7856 - mae: 5.1043 - val_loss: 80.8619 - val_mae: 6.4714\n",
      "Epoch 36/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 67.1837 - mae: 5.1820 - val_loss: 76.5639 - val_mae: 5.8540\n",
      "Epoch 37/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 66.0553 - mae: 5.0691 - val_loss: 76.4913 - val_mae: 5.8815\n",
      "Epoch 38/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 64.6107 - mae: 5.0375 - val_loss: 75.5540 - val_mae: 5.7235\n",
      "Epoch 39/200\n",
      "2526/2526 [==============================] - 0s 169us/step - loss: 64.2798 - mae: 4.9862 - val_loss: 82.8058 - val_mae: 5.7261\n",
      "Epoch 40/200\n",
      "2526/2526 [==============================] - 0s 157us/step - loss: 64.1587 - mae: 5.0218 - val_loss: 78.7818 - val_mae: 5.6838\n",
      "Epoch 41/200\n",
      "2526/2526 [==============================] - 0s 162us/step - loss: 64.0606 - mae: 5.0357 - val_loss: 82.1459 - val_mae: 5.7879\n",
      "Epoch 42/200\n",
      "2526/2526 [==============================] - 0s 162us/step - loss: 63.0804 - mae: 4.9451 - val_loss: 77.5766 - val_mae: 5.8076\n",
      "Epoch 43/200\n",
      "2526/2526 [==============================] - 0s 162us/step - loss: 63.8175 - mae: 5.0642 - val_loss: 77.7797 - val_mae: 5.9164\n",
      "Epoch 44/200\n",
      "2526/2526 [==============================] - 0s 165us/step - loss: 61.0449 - mae: 4.8327 - val_loss: 77.5843 - val_mae: 5.8483\n",
      "Epoch 45/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 61.1122 - mae: 4.9276 - val_loss: 80.5584 - val_mae: 5.7107\n",
      "Epoch 46/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 61.1323 - mae: 4.8883 - val_loss: 84.1951 - val_mae: 6.6400\n",
      "Epoch 47/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 60.7359 - mae: 4.9353 - val_loss: 82.9364 - val_mae: 5.8465\n",
      "Epoch 48/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 59.0527 - mae: 4.8010 - val_loss: 81.3601 - val_mae: 5.8567\n",
      "Epoch 49/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 59.0780 - mae: 4.8503 - val_loss: 79.8244 - val_mae: 5.7759\n",
      "Epoch 50/200\n",
      "2526/2526 [==============================] - ETA: 0s - loss: 57.9427 - mae: 4.67 - 0s 136us/step - loss: 57.6218 - mae: 4.7176 - val_loss: 83.5776 - val_mae: 6.4214\n",
      "Epoch 51/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 57.5319 - mae: 4.7222 - val_loss: 80.8303 - val_mae: 5.8523\n",
      "Epoch 52/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 57.1819 - mae: 4.6491 - val_loss: 86.6189 - val_mae: 6.6459\n",
      "Epoch 53/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 57.0981 - mae: 4.7458 - val_loss: 93.7753 - val_mae: 7.4100\n",
      "Epoch 54/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 56.5865 - mae: 4.7057 - val_loss: 80.8553 - val_mae: 6.0888\n",
      "Epoch 55/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 55.6744 - mae: 4.6476 - val_loss: 82.0783 - val_mae: 6.3034\n",
      "Epoch 56/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 55.0258 - mae: 4.6064 - val_loss: 82.5336 - val_mae: 6.3412\n",
      "Epoch 57/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 54.8104 - mae: 4.6316 - val_loss: 85.9348 - val_mae: 6.5328\n",
      "Epoch 58/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 56.2334 - mae: 4.7296 - val_loss: 83.2682 - val_mae: 6.1721\n",
      "Epoch 59/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 55.0636 - mae: 4.7400 - val_loss: 89.0646 - val_mae: 6.0740\n",
      "Epoch 60/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 52.0435 - mae: 4.4220 - val_loss: 89.2288 - val_mae: 6.8055\n",
      "Epoch 61/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 52.5133 - mae: 4.5207 - val_loss: 88.7440 - val_mae: 6.1161\n",
      "Epoch 62/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 51.3478 - mae: 4.3712 - val_loss: 85.3861 - val_mae: 6.3576\n",
      "Epoch 63/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 51.1275 - mae: 4.4453 - val_loss: 87.5417 - val_mae: 6.4341\n",
      "Epoch 64/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 52.4658 - mae: 4.5117 - val_loss: 91.8845 - val_mae: 6.1504\n",
      "Epoch 65/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 49.9429 - mae: 4.3451 - val_loss: 85.2324 - val_mae: 6.2467\n",
      "Epoch 66/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 49.0446 - mae: 4.3096 - val_loss: 87.9523 - val_mae: 6.5406\n",
      "Epoch 67/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 50.3121 - mae: 4.4066 - val_loss: 90.8767 - val_mae: 6.7144\n",
      "Epoch 68/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 49.0679 - mae: 4.3110 - val_loss: 87.4179 - val_mae: 6.0571\n",
      "Epoch 69/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 48.5126 - mae: 4.3064 - val_loss: 91.6591 - val_mae: 6.7423\n",
      "Epoch 70/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 48.5070 - mae: 4.2693 - val_loss: 93.5017 - val_mae: 6.3200\n",
      "Epoch 71/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 50.9155 - mae: 4.5138 - val_loss: 90.4806 - val_mae: 6.4944\n",
      "Epoch 72/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 46.8616 - mae: 4.2355 - val_loss: 88.8445 - val_mae: 6.2404\n",
      "Epoch 73/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 46.3555 - mae: 4.1573 - val_loss: 90.4981 - val_mae: 6.4562\n",
      "Epoch 74/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 46.1911 - mae: 4.1316 - val_loss: 91.4284 - val_mae: 6.6787\n",
      "Epoch 75/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 45.7063 - mae: 4.1297 - val_loss: 91.6247 - val_mae: 6.2980\n",
      "Epoch 76/200\n",
      "2526/2526 [==============================] - 0s 161us/step - loss: 46.0385 - mae: 4.1714 - val_loss: 89.7399 - val_mae: 6.3350\n",
      "Epoch 77/200\n",
      "2526/2526 [==============================] - 0s 165us/step - loss: 45.1904 - mae: 4.1160 - val_loss: 91.4987 - val_mae: 6.4050\n",
      "Epoch 78/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 44.0423 - mae: 4.0332 - val_loss: 93.5880 - val_mae: 6.4778\n",
      "Epoch 79/200\n",
      "2526/2526 [==============================] - 0s 176us/step - loss: 45.5678 - mae: 4.1161 - val_loss: 94.3725 - val_mae: 6.5642\n",
      "Epoch 80/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 44.1203 - mae: 4.0357 - val_loss: 94.0566 - val_mae: 6.6366\n",
      "Epoch 81/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 43.8213 - mae: 4.0750 - val_loss: 96.9606 - val_mae: 6.9256\n",
      "Epoch 82/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 44.2034 - mae: 4.0774 - val_loss: 94.6260 - val_mae: 6.6583\n",
      "Epoch 83/200\n",
      "2526/2526 [==============================] - 1s 225us/step - loss: 43.0862 - mae: 4.0359 - val_loss: 103.7183 - val_mae: 7.5018\n",
      "Epoch 84/200\n",
      "2526/2526 [==============================] - 0s 183us/step - loss: 43.3070 - mae: 4.0693 - val_loss: 95.8598 - val_mae: 6.8443\n",
      "Epoch 85/200\n",
      "2526/2526 [==============================] - 1s 334us/step - loss: 43.2091 - mae: 4.0495 - val_loss: 95.1906 - val_mae: 6.6176\n",
      "Epoch 86/200\n",
      "2526/2526 [==============================] - 1s 296us/step - loss: 42.2823 - mae: 3.9567 - val_loss: 95.5467 - val_mae: 6.7916\n",
      "Epoch 87/200\n",
      "2526/2526 [==============================] - 1s 268us/step - loss: 41.9826 - mae: 3.9309 - val_loss: 97.2355 - val_mae: 6.6480\n",
      "Epoch 88/200\n",
      "2526/2526 [==============================] - 1s 218us/step - loss: 40.8137 - mae: 3.7944 - val_loss: 97.2019 - val_mae: 6.6725\n",
      "Epoch 89/200\n",
      "2526/2526 [==============================] - 1s 216us/step - loss: 41.6025 - mae: 3.9066 - val_loss: 104.4289 - val_mae: 7.4390\n",
      "Epoch 90/200\n",
      "2526/2526 [==============================] - 0s 186us/step - loss: 43.2832 - mae: 4.1479 - val_loss: 98.0838 - val_mae: 6.6273\n",
      "Epoch 91/200\n",
      "2526/2526 [==============================] - 1s 307us/step - loss: 40.8943 - mae: 3.9385 - val_loss: 99.4335 - val_mae: 6.9473\n",
      "Epoch 92/200\n",
      "2526/2526 [==============================] - 1s 272us/step - loss: 40.2247 - mae: 3.8601 - val_loss: 98.7812 - val_mae: 6.8288\n",
      "Epoch 93/200\n",
      "2526/2526 [==============================] - 0s 159us/step - loss: 39.7946 - mae: 3.8123 - val_loss: 99.4227 - val_mae: 6.7822\n",
      "Epoch 94/200\n",
      "2526/2526 [==============================] - 0s 153us/step - loss: 40.1446 - mae: 3.8106 - val_loss: 100.2678 - val_mae: 6.9973\n",
      "Epoch 95/200\n",
      "2526/2526 [==============================] - 0s 195us/step - loss: 40.7737 - mae: 3.9776 - val_loss: 99.8225 - val_mae: 7.0817\n",
      "Epoch 96/200\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 40.0186 - mae: 3.9291 - val_loss: 98.6730 - val_mae: 6.6772\n",
      "Epoch 97/200\n",
      "2526/2526 [==============================] - 0s 160us/step - loss: 38.5299 - mae: 3.7377 - val_loss: 101.4309 - val_mae: 6.8214\n",
      "Epoch 98/200\n",
      "2526/2526 [==============================] - 0s 168us/step - loss: 39.9502 - mae: 3.9255 - val_loss: 104.1367 - val_mae: 7.2425\n",
      "Epoch 99/200\n",
      "2526/2526 [==============================] - 0s 165us/step - loss: 38.3541 - mae: 3.7516 - val_loss: 104.9918 - val_mae: 7.0040\n",
      "Epoch 100/200\n",
      "2526/2526 [==============================] - 0s 161us/step - loss: 41.2147 - mae: 4.0599 - val_loss: 101.5433 - val_mae: 6.7506\n",
      "Epoch 101/200\n",
      "2526/2526 [==============================] - 0s 162us/step - loss: 38.8498 - mae: 3.8063 - val_loss: 103.2919 - val_mae: 7.0597\n",
      "Epoch 102/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 37.5474 - mae: 3.7095 - val_loss: 103.7526 - val_mae: 7.1321\n",
      "Epoch 103/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 37.6430 - mae: 3.6925 - val_loss: 104.1816 - val_mae: 7.2166\n",
      "Epoch 104/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 36.6565 - mae: 3.6341 - val_loss: 102.7794 - val_mae: 7.0585\n",
      "Epoch 105/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 36.7269 - mae: 3.6002 - val_loss: 105.5172 - val_mae: 7.2464\n",
      "Epoch 106/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 36.7767 - mae: 3.7101 - val_loss: 110.5474 - val_mae: 7.0989\n",
      "Epoch 107/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 38.4359 - mae: 3.8810 - val_loss: 104.2540 - val_mae: 6.9554\n",
      "Epoch 108/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 36.6176 - mae: 3.6255 - val_loss: 111.2529 - val_mae: 7.5879\n",
      "Epoch 109/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 38.7923 - mae: 3.8787 - val_loss: 105.8621 - val_mae: 7.0300\n",
      "Epoch 110/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 35.8244 - mae: 3.6156 - val_loss: 103.7567 - val_mae: 6.9710\n",
      "Epoch 111/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 34.9488 - mae: 3.5151 - val_loss: 106.3695 - val_mae: 7.0501\n",
      "Epoch 112/200\n",
      "2526/2526 [==============================] - 0s 149us/step - loss: 36.5862 - mae: 3.7144 - val_loss: 103.7381 - val_mae: 7.0586\n",
      "Epoch 113/200\n",
      "2526/2526 [==============================] - 0s 155us/step - loss: 35.2089 - mae: 3.6427 - val_loss: 105.1998 - val_mae: 6.9543\n",
      "Epoch 114/200\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 35.6161 - mae: 3.6231 - val_loss: 107.7807 - val_mae: 7.1603\n",
      "Epoch 115/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 34.3375 - mae: 3.5271 - val_loss: 105.6499 - val_mae: 6.9839\n",
      "Epoch 116/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 34.5104 - mae: 3.5489 - val_loss: 106.0783 - val_mae: 7.1817\n",
      "Epoch 117/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 34.9468 - mae: 3.5846 - val_loss: 104.6302 - val_mae: 7.1544\n",
      "Epoch 118/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 33.9916 - mae: 3.5453 - val_loss: 112.3784 - val_mae: 7.5954\n",
      "Epoch 119/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 34.6412 - mae: 3.5780 - val_loss: 111.3161 - val_mae: 7.4022\n",
      "Epoch 120/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 34.6281 - mae: 3.5602 - val_loss: 109.4742 - val_mae: 7.1740\n",
      "Epoch 121/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 33.5657 - mae: 3.4985 - val_loss: 108.8142 - val_mae: 7.2655\n",
      "Epoch 122/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 32.7739 - mae: 3.3937 - val_loss: 108.9049 - val_mae: 7.2815\n",
      "Epoch 123/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 33.8651 - mae: 3.5091 - val_loss: 112.4796 - val_mae: 7.3931\n",
      "Epoch 124/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 33.5662 - mae: 3.5954 - val_loss: 112.2546 - val_mae: 7.2673\n",
      "Epoch 125/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 33.4302 - mae: 3.5396 - val_loss: 107.6267 - val_mae: 7.1150\n",
      "Epoch 126/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 32.3009 - mae: 3.4201 - val_loss: 109.8515 - val_mae: 7.2488\n",
      "Epoch 127/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 32.2572 - mae: 3.4048 - val_loss: 111.9976 - val_mae: 7.4836\n",
      "Epoch 128/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 32.3305 - mae: 3.3831 - val_loss: 112.5798 - val_mae: 7.4830\n",
      "Epoch 129/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 31.7086 - mae: 3.3864 - val_loss: 111.4643 - val_mae: 7.2351\n",
      "Epoch 130/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 31.7705 - mae: 3.3497 - val_loss: 114.5544 - val_mae: 7.5541\n",
      "Epoch 131/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 31.8222 - mae: 3.4241 - val_loss: 113.6061 - val_mae: 7.5409\n",
      "Epoch 132/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 33.0670 - mae: 3.6564 - val_loss: 117.0130 - val_mae: 7.8312\n",
      "Epoch 133/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 30.8994 - mae: 3.4375 - val_loss: 115.8555 - val_mae: 7.5252\n",
      "Epoch 134/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 31.1209 - mae: 3.3608 - val_loss: 113.0837 - val_mae: 7.2720\n",
      "Epoch 135/200\n",
      "2526/2526 [==============================] - 0s 149us/step - loss: 31.7287 - mae: 3.3852 - val_loss: 115.1156 - val_mae: 7.5603\n",
      "Epoch 136/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 31.6519 - mae: 3.4480 - val_loss: 116.2782 - val_mae: 7.7196\n",
      "Epoch 137/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 31.1769 - mae: 3.3896 - val_loss: 116.6499 - val_mae: 7.3107\n",
      "Epoch 138/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 30.8063 - mae: 3.3137 - val_loss: 113.1354 - val_mae: 7.2734\n",
      "Epoch 139/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 31.7975 - mae: 3.4789 - val_loss: 114.7292 - val_mae: 7.2705\n",
      "Epoch 140/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 31.1027 - mae: 3.4519 - val_loss: 116.3471 - val_mae: 7.3517\n",
      "Epoch 141/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 31.5126 - mae: 3.4291 - val_loss: 114.9157 - val_mae: 7.4264\n",
      "Epoch 142/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 30.7488 - mae: 3.3773 - val_loss: 117.9973 - val_mae: 7.5321\n",
      "Epoch 143/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 29.9439 - mae: 3.3576 - val_loss: 113.6744 - val_mae: 7.4321\n",
      "Epoch 144/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 30.3405 - mae: 3.3889 - val_loss: 112.9824 - val_mae: 7.3422\n",
      "Epoch 145/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 29.3889 - mae: 3.2964 - val_loss: 120.4409 - val_mae: 7.5767\n",
      "Epoch 146/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 29.2582 - mae: 3.3464 - val_loss: 121.0807 - val_mae: 7.5111\n",
      "Epoch 147/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 29.7160 - mae: 3.3358 - val_loss: 115.9556 - val_mae: 7.4816\n",
      "Epoch 148/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 29.9363 - mae: 3.3996 - val_loss: 117.3995 - val_mae: 7.4216\n",
      "Epoch 149/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 28.4782 - mae: 3.2258 - val_loss: 118.9815 - val_mae: 7.6098\n",
      "Epoch 150/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 28.0711 - mae: 3.1969 - val_loss: 117.3049 - val_mae: 7.4811\n",
      "Epoch 151/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 28.9830 - mae: 3.2944 - val_loss: 115.6969 - val_mae: 7.4422\n",
      "Epoch 152/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 28.3560 - mae: 3.1770 - val_loss: 118.2838 - val_mae: 7.4473\n",
      "Epoch 153/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 28.9888 - mae: 3.3352 - val_loss: 118.3506 - val_mae: 7.4823\n",
      "Epoch 154/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 29.6570 - mae: 3.3025 - val_loss: 120.2741 - val_mae: 7.5350\n",
      "Epoch 155/200\n",
      "2526/2526 [==============================] - 0s 149us/step - loss: 27.0182 - mae: 3.1187 - val_loss: 122.5868 - val_mae: 7.6586\n",
      "Epoch 156/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 27.8617 - mae: 3.1842 - val_loss: 119.8255 - val_mae: 7.6116\n",
      "Epoch 157/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 27.4156 - mae: 3.1389 - val_loss: 124.8891 - val_mae: 7.6889\n",
      "Epoch 158/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 27.3494 - mae: 3.1373 - val_loss: 126.3523 - val_mae: 7.8532\n",
      "Epoch 159/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 27.0214 - mae: 3.1255 - val_loss: 117.6340 - val_mae: 7.4493\n",
      "Epoch 160/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 26.5914 - mae: 3.1328 - val_loss: 123.3908 - val_mae: 7.7149\n",
      "Epoch 161/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 26.9754 - mae: 3.1274 - val_loss: 136.0289 - val_mae: 8.3984\n",
      "Epoch 162/200\n",
      "2526/2526 [==============================] - 0s 153us/step - loss: 27.5588 - mae: 3.2298 - val_loss: 120.3198 - val_mae: 7.7079\n",
      "Epoch 163/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 26.0420 - mae: 3.0796 - val_loss: 121.3894 - val_mae: 7.6312\n",
      "Epoch 164/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 26.8164 - mae: 3.1726 - val_loss: 123.1722 - val_mae: 7.6889\n",
      "Epoch 165/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 26.7642 - mae: 3.1602 - val_loss: 121.6723 - val_mae: 7.5855\n",
      "Epoch 166/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 26.7881 - mae: 3.1430 - val_loss: 125.9284 - val_mae: 7.7522\n",
      "Epoch 167/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 25.2616 - mae: 3.0052 - val_loss: 123.3910 - val_mae: 7.7866\n",
      "Epoch 168/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 25.3923 - mae: 3.0175 - val_loss: 126.2792 - val_mae: 7.7991\n",
      "Epoch 169/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 25.7277 - mae: 3.1125 - val_loss: 127.2987 - val_mae: 7.7884\n",
      "Epoch 170/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 26.5936 - mae: 3.2785 - val_loss: 123.3033 - val_mae: 8.0027\n",
      "Epoch 171/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 26.9363 - mae: 3.2649 - val_loss: 126.6054 - val_mae: 7.8496\n",
      "Epoch 172/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 25.5282 - mae: 3.0905 - val_loss: 129.1655 - val_mae: 7.8660\n",
      "Epoch 173/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 25.1274 - mae: 3.0698 - val_loss: 122.0564 - val_mae: 7.6239\n",
      "Epoch 174/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 26.1548 - mae: 3.2186 - val_loss: 125.8226 - val_mae: 7.6753\n",
      "Epoch 175/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 26.3962 - mae: 3.1882 - val_loss: 132.6913 - val_mae: 8.0092\n",
      "Epoch 176/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 24.9883 - mae: 3.0984 - val_loss: 126.1240 - val_mae: 7.9108\n",
      "Epoch 177/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 25.4158 - mae: 3.0925 - val_loss: 125.7621 - val_mae: 7.8250\n",
      "Epoch 178/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 24.1449 - mae: 2.9665 - val_loss: 120.9815 - val_mae: 7.5983\n",
      "Epoch 179/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 24.0396 - mae: 2.9496 - val_loss: 127.2263 - val_mae: 7.8945\n",
      "Epoch 180/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 24.9724 - mae: 3.0235 - val_loss: 127.0337 - val_mae: 7.8626\n",
      "Epoch 181/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 25.4185 - mae: 3.0926 - val_loss: 127.2572 - val_mae: 7.9925\n",
      "Epoch 182/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 25.1857 - mae: 3.0775 - val_loss: 130.8745 - val_mae: 7.9876\n",
      "Epoch 183/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 23.9109 - mae: 2.9452 - val_loss: 134.1577 - val_mae: 8.3642\n",
      "Epoch 184/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 25.3387 - mae: 3.1690 - val_loss: 121.8885 - val_mae: 7.6209\n",
      "Epoch 185/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 24.3474 - mae: 3.0457 - val_loss: 125.8368 - val_mae: 7.7915\n",
      "Epoch 186/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 23.6710 - mae: 2.9413 - val_loss: 129.0680 - val_mae: 7.8892\n",
      "Epoch 187/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 24.0411 - mae: 2.9964 - val_loss: 138.2530 - val_mae: 8.5221\n",
      "Epoch 188/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 23.1840 - mae: 2.9629 - val_loss: 133.6665 - val_mae: 8.1211\n",
      "Epoch 189/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 24.1638 - mae: 3.0162 - val_loss: 140.0228 - val_mae: 8.3757\n",
      "Epoch 190/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 23.4873 - mae: 3.0004 - val_loss: 132.0094 - val_mae: 8.0559\n",
      "Epoch 191/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 22.6664 - mae: 2.8642 - val_loss: 136.9568 - val_mae: 8.3271\n",
      "Epoch 192/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 23.0113 - mae: 2.8901 - val_loss: 124.4311 - val_mae: 7.8938\n",
      "Epoch 193/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 23.4679 - mae: 2.9524 - val_loss: 134.2528 - val_mae: 8.2639\n",
      "Epoch 194/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 23.2358 - mae: 3.0017 - val_loss: 126.9745 - val_mae: 7.8261\n",
      "Epoch 195/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 22.7861 - mae: 2.8956 - val_loss: 145.3736 - val_mae: 8.7045\n",
      "Epoch 196/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 22.6912 - mae: 2.9433 - val_loss: 136.1362 - val_mae: 8.3081\n",
      "Epoch 197/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 22.3853 - mae: 2.9679 - val_loss: 131.9466 - val_mae: 8.2292\n",
      "Epoch 198/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 23.0171 - mae: 2.9706 - val_loss: 125.3992 - val_mae: 7.8220\n",
      "Epoch 199/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 22.9233 - mae: 2.9586 - val_loss: 125.5322 - val_mae: 7.8087\n",
      "Epoch 200/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 22.4250 - mae: 2.9087 - val_loss: 133.8345 - val_mae: 8.0420\n",
      "processing fold # 1\n",
      "Train on 2526 samples, validate on 842 samples\n",
      "Epoch 1/200\n",
      "2526/2526 [==============================] - 1s 208us/step - loss: 4577.0910 - mae: 57.0275 - val_loss: 323.8926 - val_mae: 13.1712\n",
      "Epoch 2/200\n",
      "2526/2526 [==============================] - 0s 149us/step - loss: 177.8146 - mae: 10.1113 - val_loss: 215.8766 - val_mae: 9.6771\n",
      "Epoch 3/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 111.7655 - mae: 7.8043 - val_loss: 162.6709 - val_mae: 8.1593\n",
      "Epoch 4/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 87.4283 - mae: 6.7214 - val_loss: 146.3158 - val_mae: 7.0987\n",
      "Epoch 5/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 76.2336 - mae: 6.1469 - val_loss: 137.3163 - val_mae: 6.6757\n",
      "Epoch 6/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 71.8112 - mae: 5.8563 - val_loss: 132.7876 - val_mae: 6.4736\n",
      "Epoch 7/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 68.0800 - mae: 5.6885 - val_loss: 129.8377 - val_mae: 6.6567\n",
      "Epoch 8/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 65.9880 - mae: 5.5948 - val_loss: 130.3310 - val_mae: 6.3891\n",
      "Epoch 9/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 64.7683 - mae: 5.5348 - val_loss: 129.4847 - val_mae: 6.2185\n",
      "Epoch 10/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 62.6922 - mae: 5.3931 - val_loss: 129.4526 - val_mae: 6.8668\n",
      "Epoch 11/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 62.1317 - mae: 5.4269 - val_loss: 127.4938 - val_mae: 6.3269\n",
      "Epoch 12/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 61.3019 - mae: 5.3396 - val_loss: 135.3717 - val_mae: 6.0502\n",
      "Epoch 13/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 61.1130 - mae: 5.3533 - val_loss: 128.0984 - val_mae: 6.1939\n",
      "Epoch 14/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 59.5937 - mae: 5.3044 - val_loss: 127.8116 - val_mae: 6.4759\n",
      "Epoch 15/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 59.3787 - mae: 5.2763 - val_loss: 129.7920 - val_mae: 6.0872\n",
      "Epoch 16/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 58.7808 - mae: 5.2520 - val_loss: 127.9928 - val_mae: 6.2426\n",
      "Epoch 17/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 57.7497 - mae: 5.1851 - val_loss: 127.6803 - val_mae: 6.5786\n",
      "Epoch 18/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 57.8717 - mae: 5.2358 - val_loss: 127.8167 - val_mae: 6.3531\n",
      "Epoch 19/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 56.7258 - mae: 5.1452 - val_loss: 129.7410 - val_mae: 6.6624\n",
      "Epoch 20/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 56.3449 - mae: 5.1930 - val_loss: 129.2418 - val_mae: 6.0716\n",
      "Epoch 21/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 55.1257 - mae: 5.1034 - val_loss: 129.8265 - val_mae: 6.1453\n",
      "Epoch 22/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 54.5512 - mae: 5.0134 - val_loss: 134.7733 - val_mae: 7.5186\n",
      "Epoch 23/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 55.8669 - mae: 5.1878 - val_loss: 142.7978 - val_mae: 6.1559\n",
      "Epoch 24/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 53.4429 - mae: 5.0276 - val_loss: 129.5493 - val_mae: 6.2083\n",
      "Epoch 25/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 52.6666 - mae: 4.9422 - val_loss: 128.6731 - val_mae: 6.0362\n",
      "Epoch 26/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 53.6566 - mae: 5.0403 - val_loss: 130.6096 - val_mae: 6.1435\n",
      "Epoch 27/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 52.0389 - mae: 4.9874 - val_loss: 131.4518 - val_mae: 6.0403\n",
      "Epoch 28/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 52.1825 - mae: 4.9324 - val_loss: 129.7488 - val_mae: 6.1748\n",
      "Epoch 29/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 51.1231 - mae: 4.9054 - val_loss: 127.2706 - val_mae: 6.6368\n",
      "Epoch 30/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 50.4103 - mae: 4.8608 - val_loss: 132.7897 - val_mae: 6.1537\n",
      "Epoch 31/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 49.8165 - mae: 4.8153 - val_loss: 127.9507 - val_mae: 6.3325\n",
      "Epoch 32/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 49.7543 - mae: 4.8420 - val_loss: 129.6872 - val_mae: 6.3857\n",
      "Epoch 33/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 49.0724 - mae: 4.8062 - val_loss: 130.4978 - val_mae: 6.4242\n",
      "Epoch 34/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 48.2285 - mae: 4.7362 - val_loss: 129.4965 - val_mae: 6.8188\n",
      "Epoch 35/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 47.9835 - mae: 4.7476 - val_loss: 128.7038 - val_mae: 6.4169\n",
      "Epoch 36/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 47.6517 - mae: 4.7250 - val_loss: 133.7095 - val_mae: 7.3448\n",
      "Epoch 37/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 47.7797 - mae: 4.7446 - val_loss: 135.2023 - val_mae: 7.5320\n",
      "Epoch 38/200\n",
      "2526/2526 [==============================] - 0s 178us/step - loss: 46.9769 - mae: 4.6889 - val_loss: 128.8901 - val_mae: 6.3558\n",
      "Epoch 39/200\n",
      "2526/2526 [==============================] - 0s 156us/step - loss: 45.7603 - mae: 4.6410 - val_loss: 130.6886 - val_mae: 6.4175\n",
      "Epoch 40/200\n",
      "2526/2526 [==============================] - 0s 154us/step - loss: 45.0156 - mae: 4.5920 - val_loss: 130.7258 - val_mae: 6.5693\n",
      "Epoch 41/200\n",
      "2526/2526 [==============================] - 0s 173us/step - loss: 45.5053 - mae: 4.6517 - val_loss: 130.0336 - val_mae: 6.4839\n",
      "Epoch 42/200\n",
      "2526/2526 [==============================] - 0s 161us/step - loss: 44.7876 - mae: 4.5536 - val_loss: 130.9008 - val_mae: 6.8152\n",
      "Epoch 43/200\n",
      "2526/2526 [==============================] - 0s 154us/step - loss: 44.2014 - mae: 4.5623 - val_loss: 137.2177 - val_mae: 6.2927\n",
      "Epoch 44/200\n",
      "2526/2526 [==============================] - 0s 156us/step - loss: 43.7329 - mae: 4.5314 - val_loss: 132.7146 - val_mae: 6.3057\n",
      "Epoch 45/200\n",
      "2526/2526 [==============================] - 0s 156us/step - loss: 43.3943 - mae: 4.5016 - val_loss: 130.9637 - val_mae: 6.7279\n",
      "Epoch 46/200\n",
      "2526/2526 [==============================] - 0s 155us/step - loss: 43.0773 - mae: 4.4974 - val_loss: 134.6629 - val_mae: 7.2875\n",
      "Epoch 47/200\n",
      "2526/2526 [==============================] - 0s 154us/step - loss: 41.8597 - mae: 4.4216 - val_loss: 133.0682 - val_mae: 7.0481\n",
      "Epoch 48/200\n",
      "2526/2526 [==============================] - 0s 171us/step - loss: 41.2049 - mae: 4.3729 - val_loss: 134.5943 - val_mae: 6.3318\n",
      "Epoch 49/200\n",
      "2526/2526 [==============================] - 0s 172us/step - loss: 42.5513 - mae: 4.4680 - val_loss: 134.4471 - val_mae: 7.1255\n",
      "Epoch 50/200\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 39.8534 - mae: 4.3046 - val_loss: 133.3448 - val_mae: 6.7697\n",
      "Epoch 51/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 40.8486 - mae: 4.3759 - val_loss: 136.8069 - val_mae: 7.2360\n",
      "Epoch 52/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 40.1530 - mae: 4.3294 - val_loss: 132.5575 - val_mae: 6.7078\n",
      "Epoch 53/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 38.1377 - mae: 4.1742 - val_loss: 133.7417 - val_mae: 6.9224\n",
      "Epoch 54/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 40.3746 - mae: 4.3817 - val_loss: 135.4260 - val_mae: 6.6101\n",
      "Epoch 55/200\n",
      "2526/2526 [==============================] - 0s 134us/step - loss: 38.7120 - mae: 4.2375 - val_loss: 133.4944 - val_mae: 6.6536\n",
      "Epoch 56/200\n",
      "2526/2526 [==============================] - 0s 134us/step - loss: 38.4235 - mae: 4.1919 - val_loss: 135.8389 - val_mae: 7.1880\n",
      "Epoch 57/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 39.2390 - mae: 4.2753 - val_loss: 135.7375 - val_mae: 6.6599\n",
      "Epoch 58/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 37.1075 - mae: 4.1183 - val_loss: 136.9969 - val_mae: 6.7035\n",
      "Epoch 59/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 36.0119 - mae: 4.0568 - val_loss: 140.3691 - val_mae: 7.5868\n",
      "Epoch 60/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 37.6207 - mae: 4.1817 - val_loss: 135.2932 - val_mae: 6.9164\n",
      "Epoch 61/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 36.2121 - mae: 4.0793 - val_loss: 135.0964 - val_mae: 7.0417\n",
      "Epoch 62/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 35.4990 - mae: 4.0557 - val_loss: 137.0561 - val_mae: 7.1203\n",
      "Epoch 63/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 35.0364 - mae: 3.9663 - val_loss: 135.9801 - val_mae: 6.9229\n",
      "Epoch 64/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 34.5777 - mae: 3.9896 - val_loss: 146.2312 - val_mae: 6.5975\n",
      "Epoch 65/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 34.4725 - mae: 3.9600 - val_loss: 137.0752 - val_mae: 6.8638\n",
      "Epoch 66/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 33.7596 - mae: 3.9112 - val_loss: 137.4600 - val_mae: 6.9807\n",
      "Epoch 67/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 34.5833 - mae: 3.9507 - val_loss: 141.7066 - val_mae: 7.5428\n",
      "Epoch 68/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 34.0276 - mae: 3.9219 - val_loss: 138.4092 - val_mae: 6.8789\n",
      "Epoch 69/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 33.5145 - mae: 3.9038 - val_loss: 138.7981 - val_mae: 6.9429\n",
      "Epoch 70/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 32.6262 - mae: 3.8425 - val_loss: 143.0829 - val_mae: 6.9721\n",
      "Epoch 71/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 33.2461 - mae: 3.8608 - val_loss: 140.6495 - val_mae: 7.4676\n",
      "Epoch 72/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 32.5978 - mae: 3.9069 - val_loss: 140.3959 - val_mae: 7.2085\n",
      "Epoch 73/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 31.9692 - mae: 3.7786 - val_loss: 139.9143 - val_mae: 7.2370\n",
      "Epoch 74/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 31.2445 - mae: 3.7295 - val_loss: 142.0607 - val_mae: 7.4740\n",
      "Epoch 75/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 31.6247 - mae: 3.7899 - val_loss: 139.0015 - val_mae: 7.0592\n",
      "Epoch 76/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 30.9347 - mae: 3.7219 - val_loss: 140.2723 - val_mae: 6.7381\n",
      "Epoch 77/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 31.0031 - mae: 3.7497 - val_loss: 138.1585 - val_mae: 6.8741\n",
      "Epoch 78/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 30.7956 - mae: 3.7074 - val_loss: 139.9243 - val_mae: 7.0180\n",
      "Epoch 79/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 31.1829 - mae: 3.7579 - val_loss: 142.9507 - val_mae: 7.0861\n",
      "Epoch 80/200\n",
      "2526/2526 [==============================] - 0s 148us/step - loss: 29.9230 - mae: 3.6125 - val_loss: 140.1990 - val_mae: 7.2549\n",
      "Epoch 81/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 29.2268 - mae: 3.5587 - val_loss: 141.9561 - val_mae: 7.1900\n",
      "Epoch 82/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 29.1398 - mae: 3.5984 - val_loss: 138.3534 - val_mae: 6.9887\n",
      "Epoch 83/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 28.9760 - mae: 3.5616 - val_loss: 142.8733 - val_mae: 7.3496\n",
      "Epoch 84/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 29.7688 - mae: 3.6205 - val_loss: 142.3052 - val_mae: 7.0818\n",
      "Epoch 85/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 28.6400 - mae: 3.5230 - val_loss: 143.9107 - val_mae: 6.9034\n",
      "Epoch 86/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 28.8197 - mae: 3.5031 - val_loss: 142.1664 - val_mae: 7.3983\n",
      "Epoch 87/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 28.7671 - mae: 3.5682 - val_loss: 142.8728 - val_mae: 6.8696\n",
      "Epoch 88/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 30.5874 - mae: 3.8037 - val_loss: 147.7591 - val_mae: 8.1458\n",
      "Epoch 89/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 27.9185 - mae: 3.4576 - val_loss: 141.6602 - val_mae: 7.1457\n",
      "Epoch 90/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 28.4096 - mae: 3.5447 - val_loss: 142.3288 - val_mae: 7.1273\n",
      "Epoch 91/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 27.3622 - mae: 3.3740 - val_loss: 143.4992 - val_mae: 7.1856\n",
      "Epoch 92/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 27.5069 - mae: 3.4718 - val_loss: 143.0084 - val_mae: 7.2211\n",
      "Epoch 93/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 26.4050 - mae: 3.3442 - val_loss: 146.7299 - val_mae: 7.0139\n",
      "Epoch 94/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 26.5556 - mae: 3.3554 - val_loss: 141.9783 - val_mae: 6.9791\n",
      "Epoch 95/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 26.4687 - mae: 3.3706 - val_loss: 141.4543 - val_mae: 7.2088\n",
      "Epoch 96/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 26.4048 - mae: 3.3118 - val_loss: 142.2403 - val_mae: 7.2600\n",
      "Epoch 97/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 26.2994 - mae: 3.3320 - val_loss: 147.9691 - val_mae: 7.9618\n",
      "Epoch 98/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 27.6112 - mae: 3.5301 - val_loss: 142.1481 - val_mae: 7.3218\n",
      "Epoch 99/200\n",
      "2526/2526 [==============================] - 0s 134us/step - loss: 26.7267 - mae: 3.4209 - val_loss: 142.2975 - val_mae: 7.2445\n",
      "Epoch 100/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 25.7929 - mae: 3.2744 - val_loss: 143.6543 - val_mae: 7.3484\n",
      "Epoch 101/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 26.7620 - mae: 3.3669 - val_loss: 142.8357 - val_mae: 7.3554\n",
      "Epoch 102/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 25.3558 - mae: 3.2801 - val_loss: 141.9591 - val_mae: 7.3654\n",
      "Epoch 103/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 26.3049 - mae: 3.4035 - val_loss: 143.2824 - val_mae: 7.2275\n",
      "Epoch 104/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 25.8293 - mae: 3.3203 - val_loss: 143.8968 - val_mae: 7.4985\n",
      "Epoch 105/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 25.0942 - mae: 3.2188 - val_loss: 142.7306 - val_mae: 7.3011\n",
      "Epoch 106/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 25.0596 - mae: 3.2565 - val_loss: 143.9023 - val_mae: 7.2768\n",
      "Epoch 107/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 25.2488 - mae: 3.2684 - val_loss: 144.4179 - val_mae: 7.2304\n",
      "Epoch 108/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 25.2954 - mae: 3.2396 - val_loss: 144.4119 - val_mae: 7.5894\n",
      "Epoch 109/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 24.4361 - mae: 3.1614 - val_loss: 142.9237 - val_mae: 7.3884\n",
      "Epoch 110/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 24.0320 - mae: 3.1438 - val_loss: 143.7293 - val_mae: 7.2850\n",
      "Epoch 111/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 24.9058 - mae: 3.2383 - val_loss: 148.4716 - val_mae: 7.7797\n",
      "Epoch 112/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 24.0771 - mae: 3.1171 - val_loss: 146.1635 - val_mae: 7.2970\n",
      "Epoch 113/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 24.5897 - mae: 3.1886 - val_loss: 145.6758 - val_mae: 7.5630\n",
      "Epoch 114/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 24.8809 - mae: 3.2488 - val_loss: 157.5425 - val_mae: 8.5933\n",
      "Epoch 115/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 24.5001 - mae: 3.2176 - val_loss: 148.1935 - val_mae: 7.9950\n",
      "Epoch 116/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 23.4931 - mae: 3.0671 - val_loss: 143.8065 - val_mae: 7.6068\n",
      "Epoch 117/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 24.0712 - mae: 3.1213 - val_loss: 145.3839 - val_mae: 7.4951\n",
      "Epoch 118/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 22.8445 - mae: 3.0173 - val_loss: 146.8207 - val_mae: 7.4234\n",
      "Epoch 119/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 24.6524 - mae: 3.2327 - val_loss: 146.7173 - val_mae: 7.4463\n",
      "Epoch 120/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 23.1674 - mae: 3.0899 - val_loss: 148.1212 - val_mae: 7.7981\n",
      "Epoch 121/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 23.8075 - mae: 3.1426 - val_loss: 150.7703 - val_mae: 8.0894\n",
      "Epoch 122/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 22.9043 - mae: 3.0655 - val_loss: 145.1230 - val_mae: 7.4261\n",
      "Epoch 123/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 22.3447 - mae: 2.9082 - val_loss: 148.9663 - val_mae: 7.8053\n",
      "Epoch 124/200\n",
      "2526/2526 [==============================] - 0s 134us/step - loss: 22.9355 - mae: 3.0741 - val_loss: 147.1756 - val_mae: 7.5958\n",
      "Epoch 125/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 23.0678 - mae: 3.0282 - val_loss: 147.6295 - val_mae: 7.5042\n",
      "Epoch 126/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 22.3242 - mae: 2.9478 - val_loss: 149.0790 - val_mae: 7.7524\n",
      "Epoch 127/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 22.2709 - mae: 2.9628 - val_loss: 145.9075 - val_mae: 7.6783\n",
      "Epoch 128/200\n",
      "2526/2526 [==============================] - 0s 153us/step - loss: 22.1821 - mae: 2.9799 - val_loss: 151.0526 - val_mae: 7.4244\n",
      "Epoch 129/200\n",
      "2526/2526 [==============================] - 1s 204us/step - loss: 23.4003 - mae: 3.1073 - val_loss: 148.1252 - val_mae: 7.7590\n",
      "Epoch 130/200\n",
      "2526/2526 [==============================] - 0s 164us/step - loss: 21.9065 - mae: 2.9305 - val_loss: 148.4219 - val_mae: 7.5796\n",
      "Epoch 131/200\n",
      "2526/2526 [==============================] - 0s 157us/step - loss: 21.8481 - mae: 2.8904 - val_loss: 148.3646 - val_mae: 7.8261\n",
      "Epoch 132/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 21.9609 - mae: 2.9039 - val_loss: 150.9558 - val_mae: 7.6023\n",
      "Epoch 133/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 21.1479 - mae: 2.8469 - val_loss: 151.7747 - val_mae: 7.5886\n",
      "Epoch 134/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 22.2040 - mae: 2.9982 - val_loss: 149.3092 - val_mae: 7.8115\n",
      "Epoch 135/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 21.6202 - mae: 2.9435 - val_loss: 150.6910 - val_mae: 7.8262\n",
      "Epoch 136/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 21.9400 - mae: 2.9304 - val_loss: 149.9071 - val_mae: 7.6274\n",
      "Epoch 137/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 22.2454 - mae: 2.9616 - val_loss: 149.5355 - val_mae: 7.5181\n",
      "Epoch 138/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 24.1737 - mae: 3.1993 - val_loss: 150.3409 - val_mae: 7.6290\n",
      "Epoch 139/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 22.6133 - mae: 3.0454 - val_loss: 151.5227 - val_mae: 7.6885\n",
      "Epoch 140/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 21.4631 - mae: 2.8975 - val_loss: 150.9025 - val_mae: 7.9024\n",
      "Epoch 141/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 22.7630 - mae: 3.0673 - val_loss: 152.8136 - val_mae: 7.6165\n",
      "Epoch 142/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 22.7618 - mae: 3.1360 - val_loss: 153.5028 - val_mae: 8.1174\n",
      "Epoch 143/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 22.0039 - mae: 2.9706 - val_loss: 153.8594 - val_mae: 7.7730\n",
      "Epoch 144/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 21.7963 - mae: 2.9778 - val_loss: 150.6883 - val_mae: 7.8731\n",
      "Epoch 145/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 21.2414 - mae: 2.9694 - val_loss: 148.8637 - val_mae: 7.7677\n",
      "Epoch 146/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 21.4836 - mae: 2.9585 - val_loss: 154.5305 - val_mae: 7.6209\n",
      "Epoch 147/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 20.9733 - mae: 2.8836 - val_loss: 152.5132 - val_mae: 7.8464\n",
      "Epoch 148/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 21.1385 - mae: 2.9131 - val_loss: 150.2601 - val_mae: 7.8270\n",
      "Epoch 149/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 21.1692 - mae: 2.8326 - val_loss: 153.8290 - val_mae: 8.0829\n",
      "Epoch 150/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 20.1918 - mae: 2.7500 - val_loss: 151.9326 - val_mae: 7.9362\n",
      "Epoch 151/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 21.4929 - mae: 2.9422 - val_loss: 156.1205 - val_mae: 8.3333\n",
      "Epoch 152/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 21.4685 - mae: 2.9049 - val_loss: 158.9903 - val_mae: 8.5361\n",
      "Epoch 153/200\n",
      "2526/2526 [==============================] - 0s 153us/step - loss: 22.0673 - mae: 3.0639 - val_loss: 151.2994 - val_mae: 7.8479\n",
      "Epoch 154/200\n",
      "2526/2526 [==============================] - 0s 154us/step - loss: 19.6906 - mae: 2.6619 - val_loss: 152.8900 - val_mae: 7.8497\n",
      "Epoch 155/200\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 20.0285 - mae: 2.7762 - val_loss: 153.1917 - val_mae: 7.8983\n",
      "Epoch 156/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 19.9001 - mae: 2.7644 - val_loss: 152.3273 - val_mae: 7.8494\n",
      "Epoch 157/200\n",
      "2526/2526 [==============================] - 0s 148us/step - loss: 19.8628 - mae: 2.7675 - val_loss: 152.4588 - val_mae: 7.7788\n",
      "Epoch 158/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 20.7240 - mae: 2.8127 - val_loss: 156.7448 - val_mae: 8.3013\n",
      "Epoch 159/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 20.6622 - mae: 2.8650 - val_loss: 152.5910 - val_mae: 7.8097\n",
      "Epoch 160/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 19.9047 - mae: 2.7172 - val_loss: 152.3586 - val_mae: 7.7294\n",
      "Epoch 161/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 20.2367 - mae: 2.7942 - val_loss: 154.3203 - val_mae: 8.0057\n",
      "Epoch 162/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 20.4635 - mae: 2.8371 - val_loss: 154.1464 - val_mae: 7.8781\n",
      "Epoch 163/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 19.5668 - mae: 2.6644 - val_loss: 155.5145 - val_mae: 7.9836\n",
      "Epoch 164/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 21.0320 - mae: 2.9477 - val_loss: 154.8533 - val_mae: 7.9730\n",
      "Epoch 165/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 19.8479 - mae: 2.7575 - val_loss: 156.9037 - val_mae: 8.3173\n",
      "Epoch 166/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 19.3860 - mae: 2.6716 - val_loss: 152.4066 - val_mae: 7.8280\n",
      "Epoch 167/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 19.5664 - mae: 2.7029 - val_loss: 155.8656 - val_mae: 8.1551\n",
      "Epoch 168/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 19.4201 - mae: 2.6824 - val_loss: 158.8155 - val_mae: 8.4160\n",
      "Epoch 169/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 19.3056 - mae: 2.7157 - val_loss: 152.7090 - val_mae: 7.9308\n",
      "Epoch 170/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 20.0599 - mae: 2.7512 - val_loss: 156.3414 - val_mae: 8.0741\n",
      "Epoch 171/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 19.6168 - mae: 2.6620 - val_loss: 157.4550 - val_mae: 8.2151\n",
      "Epoch 172/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 19.3774 - mae: 2.6564 - val_loss: 155.6898 - val_mae: 8.1833\n",
      "Epoch 173/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 18.6789 - mae: 2.5986 - val_loss: 155.3489 - val_mae: 7.9298\n",
      "Epoch 174/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 20.6267 - mae: 2.8224 - val_loss: 157.7385 - val_mae: 7.8746\n",
      "Epoch 175/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 19.5834 - mae: 2.7111 - val_loss: 157.7857 - val_mae: 8.1059\n",
      "Epoch 176/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 19.4953 - mae: 2.7191 - val_loss: 158.9203 - val_mae: 8.0198\n",
      "Epoch 177/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 20.6418 - mae: 2.8937 - val_loss: 154.6982 - val_mae: 8.1604\n",
      "Epoch 178/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 19.3335 - mae: 2.7198 - val_loss: 157.4769 - val_mae: 8.2679\n",
      "Epoch 179/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 19.2157 - mae: 2.6401 - val_loss: 158.1464 - val_mae: 8.3634\n",
      "Epoch 180/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 19.1598 - mae: 2.7038 - val_loss: 158.0489 - val_mae: 8.1754\n",
      "Epoch 181/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 20.3712 - mae: 2.8255 - val_loss: 156.3816 - val_mae: 8.0081\n",
      "Epoch 182/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 19.5953 - mae: 2.7085 - val_loss: 156.3644 - val_mae: 8.0600\n",
      "Epoch 183/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 18.9717 - mae: 2.6300 - val_loss: 160.9517 - val_mae: 8.0180\n",
      "Epoch 184/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 20.5554 - mae: 2.8747 - val_loss: 161.5194 - val_mae: 8.5123\n",
      "Epoch 185/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 18.7302 - mae: 2.6281 - val_loss: 157.4608 - val_mae: 8.0662\n",
      "Epoch 186/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 19.4747 - mae: 2.7463 - val_loss: 157.9885 - val_mae: 8.1274\n",
      "Epoch 187/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 18.3431 - mae: 2.5968 - val_loss: 156.6167 - val_mae: 8.0001\n",
      "Epoch 188/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 19.3881 - mae: 2.7396 - val_loss: 156.9685 - val_mae: 7.9820\n",
      "Epoch 189/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 18.8230 - mae: 2.6055 - val_loss: 162.4263 - val_mae: 8.5946\n",
      "Epoch 190/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 20.1188 - mae: 2.8788 - val_loss: 162.0963 - val_mae: 8.6354\n",
      "Epoch 191/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 18.1082 - mae: 2.5269 - val_loss: 158.4510 - val_mae: 7.9922\n",
      "Epoch 192/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 18.5494 - mae: 2.5728 - val_loss: 159.0454 - val_mae: 8.3304\n",
      "Epoch 193/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 18.8513 - mae: 2.6463 - val_loss: 160.4308 - val_mae: 8.2294\n",
      "Epoch 194/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 18.4934 - mae: 2.5630 - val_loss: 159.8544 - val_mae: 8.2508\n",
      "Epoch 195/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 18.5042 - mae: 2.5548 - val_loss: 157.8306 - val_mae: 8.2722\n",
      "Epoch 196/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 20.2934 - mae: 2.8221 - val_loss: 165.8273 - val_mae: 8.7456\n",
      "Epoch 197/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 19.0835 - mae: 2.6815 - val_loss: 162.4691 - val_mae: 8.0488\n",
      "Epoch 198/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 18.6458 - mae: 2.6744 - val_loss: 159.3268 - val_mae: 8.2013\n",
      "Epoch 199/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 18.7184 - mae: 2.6619 - val_loss: 161.6661 - val_mae: 8.4088\n",
      "Epoch 200/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 18.7104 - mae: 2.6419 - val_loss: 159.3978 - val_mae: 8.0363\n",
      "processing fold # 2\n",
      "Train on 2526 samples, validate on 842 samples\n",
      "Epoch 1/200\n",
      "2526/2526 [==============================] - 1s 199us/step - loss: 4702.3529 - mae: 57.8636 - val_loss: 232.7956 - val_mae: 12.1303\n",
      "Epoch 2/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 212.0206 - mae: 10.7430 - val_loss: 127.9723 - val_mae: 8.6090\n",
      "Epoch 3/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 137.7070 - mae: 8.3037 - val_loss: 95.2839 - val_mae: 7.3000\n",
      "Epoch 4/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 109.0313 - mae: 7.0590 - val_loss: 80.8145 - val_mae: 6.5007\n",
      "Epoch 5/200\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 96.6720 - mae: 6.4409 - val_loss: 76.7353 - val_mae: 6.4748\n",
      "Epoch 6/200\n",
      "2526/2526 [==============================] - 0s 153us/step - loss: 90.2588 - mae: 6.1435 - val_loss: 71.7678 - val_mae: 5.8827\n",
      "Epoch 7/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 87.8441 - mae: 6.0095 - val_loss: 70.2161 - val_mae: 5.9039\n",
      "Epoch 8/200\n",
      "2526/2526 [==============================] - 0s 178us/step - loss: 85.5966 - mae: 5.9155 - val_loss: 69.3089 - val_mae: 5.7046\n",
      "Epoch 9/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 84.0113 - mae: 5.8438 - val_loss: 68.1786 - val_mae: 5.6335\n",
      "Epoch 10/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 82.8061 - mae: 5.7668 - val_loss: 67.4569 - val_mae: 5.7596\n",
      "Epoch 11/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 81.2968 - mae: 5.6760 - val_loss: 67.1038 - val_mae: 5.6297\n",
      "Epoch 12/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 81.1293 - mae: 5.7202 - val_loss: 66.8990 - val_mae: 5.7028\n",
      "Epoch 13/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 80.3496 - mae: 5.6570 - val_loss: 67.0831 - val_mae: 5.9042\n",
      "Epoch 14/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 78.9132 - mae: 5.5778 - val_loss: 66.9145 - val_mae: 5.8435\n",
      "Epoch 15/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 79.4247 - mae: 5.6622 - val_loss: 68.5111 - val_mae: 6.1778\n",
      "Epoch 16/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 79.0630 - mae: 5.5733 - val_loss: 66.3086 - val_mae: 5.8571\n",
      "Epoch 17/200\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 77.5749 - mae: 5.5571 - val_loss: 66.8835 - val_mae: 5.9816\n",
      "Epoch 18/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 76.6753 - mae: 5.5052 - val_loss: 65.6906 - val_mae: 5.7337\n",
      "Epoch 19/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 76.3460 - mae: 5.5389 - val_loss: 67.3643 - val_mae: 5.4313\n",
      "Epoch 20/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 76.6195 - mae: 5.4905 - val_loss: 65.3036 - val_mae: 5.6847\n",
      "Epoch 21/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 75.1586 - mae: 5.4267 - val_loss: 68.7833 - val_mae: 5.3897\n",
      "Epoch 22/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 75.5103 - mae: 5.4620 - val_loss: 67.2007 - val_mae: 5.9556\n",
      "Epoch 23/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 75.1458 - mae: 5.4843 - val_loss: 65.6496 - val_mae: 5.6213\n",
      "Epoch 24/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 74.6215 - mae: 5.4254 - val_loss: 66.0852 - val_mae: 5.5059\n",
      "Epoch 25/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 72.8633 - mae: 5.3506 - val_loss: 67.3065 - val_mae: 6.1308\n",
      "Epoch 26/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 72.9526 - mae: 5.3721 - val_loss: 65.5770 - val_mae: 5.6948\n",
      "Epoch 27/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 71.7227 - mae: 5.3010 - val_loss: 66.2956 - val_mae: 5.4062\n",
      "Epoch 28/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 72.6640 - mae: 5.3974 - val_loss: 72.3754 - val_mae: 6.6353\n",
      "Epoch 29/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 71.3400 - mae: 5.2792 - val_loss: 70.7002 - val_mae: 6.5035\n",
      "Epoch 30/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 72.2675 - mae: 5.3818 - val_loss: 66.8746 - val_mae: 5.8644\n",
      "Epoch 31/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 70.0462 - mae: 5.2392 - val_loss: 65.4833 - val_mae: 5.5786\n",
      "Epoch 32/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 70.1542 - mae: 5.2802 - val_loss: 74.0533 - val_mae: 5.4249\n",
      "Epoch 33/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 69.8238 - mae: 5.2782 - val_loss: 66.1757 - val_mae: 5.5330\n",
      "Epoch 34/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 69.3162 - mae: 5.2055 - val_loss: 67.9160 - val_mae: 5.8943\n",
      "Epoch 35/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 69.6708 - mae: 5.2923 - val_loss: 67.7367 - val_mae: 6.1164\n",
      "Epoch 36/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 67.4288 - mae: 5.1374 - val_loss: 67.4996 - val_mae: 5.8469\n",
      "Epoch 37/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 66.7610 - mae: 5.1004 - val_loss: 68.9990 - val_mae: 5.4660\n",
      "Epoch 38/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 67.2431 - mae: 5.1592 - val_loss: 68.6348 - val_mae: 5.5493\n",
      "Epoch 39/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 66.4751 - mae: 5.0828 - val_loss: 83.4626 - val_mae: 7.4782\n",
      "Epoch 40/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 65.5950 - mae: 5.0971 - val_loss: 72.4965 - val_mae: 6.6122\n",
      "Epoch 41/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 64.9915 - mae: 5.0860 - val_loss: 71.5360 - val_mae: 5.4877\n",
      "Epoch 42/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 63.8540 - mae: 4.9727 - val_loss: 69.3079 - val_mae: 6.2861\n",
      "Epoch 43/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 63.2105 - mae: 4.9577 - val_loss: 67.4970 - val_mae: 5.5912\n",
      "Epoch 44/200\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 62.1650 - mae: 4.8823 - val_loss: 71.1696 - val_mae: 5.6556\n",
      "Epoch 45/200\n",
      "2526/2526 [==============================] - 0s 172us/step - loss: 63.1279 - mae: 4.9471 - val_loss: 69.3495 - val_mae: 5.9220\n",
      "Epoch 46/200\n",
      "2526/2526 [==============================] - 0s 164us/step - loss: 62.8879 - mae: 4.9288 - val_loss: 67.7054 - val_mae: 5.8419\n",
      "Epoch 47/200\n",
      "2526/2526 [==============================] - 1s 200us/step - loss: 61.1345 - mae: 4.8614 - val_loss: 72.2723 - val_mae: 6.4700\n",
      "Epoch 48/200\n",
      "2526/2526 [==============================] - 0s 175us/step - loss: 61.9925 - mae: 4.9464 - val_loss: 73.4512 - val_mae: 5.6277\n",
      "Epoch 49/200\n",
      "2526/2526 [==============================] - 0s 169us/step - loss: 60.5290 - mae: 4.8519 - val_loss: 77.6476 - val_mae: 6.9405\n",
      "Epoch 50/200\n",
      "2526/2526 [==============================] - 0s 154us/step - loss: 60.4028 - mae: 4.9028 - val_loss: 69.8954 - val_mae: 5.5703\n",
      "Epoch 51/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 58.8074 - mae: 4.7239 - val_loss: 68.7365 - val_mae: 5.7976\n",
      "Epoch 52/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 61.2123 - mae: 4.9329 - val_loss: 70.4694 - val_mae: 5.7233\n",
      "Epoch 53/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 58.3009 - mae: 4.7524 - val_loss: 73.3822 - val_mae: 6.5851\n",
      "Epoch 54/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 57.7909 - mae: 4.7502 - val_loss: 69.5465 - val_mae: 5.9754\n",
      "Epoch 55/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 57.5519 - mae: 4.7450 - val_loss: 74.4366 - val_mae: 5.7870\n",
      "Epoch 56/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 58.3136 - mae: 4.8025 - val_loss: 72.4809 - val_mae: 5.9842\n",
      "Epoch 57/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 57.3290 - mae: 4.7364 - val_loss: 72.0558 - val_mae: 5.9201\n",
      "Epoch 58/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 56.1739 - mae: 4.6349 - val_loss: 71.7403 - val_mae: 6.2326\n",
      "Epoch 59/200\n",
      "2526/2526 [==============================] - 0s 148us/step - loss: 55.4357 - mae: 4.6308 - val_loss: 74.5184 - val_mae: 6.5588\n",
      "Epoch 60/200\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 55.4981 - mae: 4.6512 - val_loss: 71.3272 - val_mae: 5.9941\n",
      "Epoch 61/200\n",
      "2526/2526 [==============================] - 0s 148us/step - loss: 54.2035 - mae: 4.5594 - val_loss: 73.5837 - val_mae: 6.1625\n",
      "Epoch 62/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 53.6507 - mae: 4.5006 - val_loss: 73.2651 - val_mae: 5.9807\n",
      "Epoch 63/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 53.5603 - mae: 4.5419 - val_loss: 74.2760 - val_mae: 5.9265\n",
      "Epoch 64/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 53.8296 - mae: 4.5808 - val_loss: 75.4270 - val_mae: 6.4836\n",
      "Epoch 65/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 52.8832 - mae: 4.4647 - val_loss: 75.4798 - val_mae: 6.5987\n",
      "Epoch 66/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 52.8030 - mae: 4.4969 - val_loss: 85.0670 - val_mae: 7.3767\n",
      "Epoch 67/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 55.5756 - mae: 4.7243 - val_loss: 78.3065 - val_mae: 5.9586\n",
      "Epoch 68/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 51.7233 - mae: 4.4888 - val_loss: 75.8296 - val_mae: 6.1278\n",
      "Epoch 69/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 50.9009 - mae: 4.3860 - val_loss: 76.3659 - val_mae: 6.6222\n",
      "Epoch 70/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 51.7993 - mae: 4.4599 - val_loss: 77.5104 - val_mae: 6.6978\n",
      "Epoch 71/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 50.6670 - mae: 4.4192 - val_loss: 77.5529 - val_mae: 6.4405\n",
      "Epoch 72/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 49.7505 - mae: 4.3662 - val_loss: 82.5957 - val_mae: 7.1162\n",
      "Epoch 73/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 51.1573 - mae: 4.4646 - val_loss: 77.9616 - val_mae: 6.6878\n",
      "Epoch 74/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 49.1254 - mae: 4.3343 - val_loss: 78.9783 - val_mae: 6.0460\n",
      "Epoch 75/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 49.6857 - mae: 4.4007 - val_loss: 80.2833 - val_mae: 6.8947\n",
      "Epoch 76/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 49.4496 - mae: 4.4438 - val_loss: 77.0454 - val_mae: 6.2959\n",
      "Epoch 77/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 49.0826 - mae: 4.3037 - val_loss: 76.8014 - val_mae: 6.4651\n",
      "Epoch 78/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 48.6164 - mae: 4.3463 - val_loss: 78.3776 - val_mae: 6.6287\n",
      "Epoch 79/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 47.7907 - mae: 4.3314 - val_loss: 81.7838 - val_mae: 6.9686\n",
      "Epoch 80/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 47.6670 - mae: 4.2738 - val_loss: 79.1197 - val_mae: 6.2904\n",
      "Epoch 81/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 47.4130 - mae: 4.2988 - val_loss: 77.8234 - val_mae: 6.4402\n",
      "Epoch 82/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 47.6551 - mae: 4.2889 - val_loss: 78.3923 - val_mae: 6.5117\n",
      "Epoch 83/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 45.5844 - mae: 4.0998 - val_loss: 79.2836 - val_mae: 6.6136\n",
      "Epoch 84/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 46.4676 - mae: 4.2195 - val_loss: 79.3244 - val_mae: 6.5899\n",
      "Epoch 85/200\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 44.7980 - mae: 4.0902 - val_loss: 78.7235 - val_mae: 6.3749\n",
      "Epoch 86/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 45.8160 - mae: 4.2017 - val_loss: 93.1908 - val_mae: 7.7846\n",
      "Epoch 87/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 48.3273 - mae: 4.5003 - val_loss: 81.2091 - val_mae: 6.3944\n",
      "Epoch 88/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 46.5002 - mae: 4.2472 - val_loss: 83.7938 - val_mae: 6.3880\n",
      "Epoch 89/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 44.8351 - mae: 4.1559 - val_loss: 80.1529 - val_mae: 6.3182\n",
      "Epoch 90/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 44.5476 - mae: 4.1354 - val_loss: 82.1464 - val_mae: 6.7619\n",
      "Epoch 91/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 43.7077 - mae: 4.0658 - val_loss: 84.8386 - val_mae: 7.0935\n",
      "Epoch 92/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 45.7344 - mae: 4.1830 - val_loss: 83.8395 - val_mae: 6.3876\n",
      "Epoch 93/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 45.1801 - mae: 4.2341 - val_loss: 84.6472 - val_mae: 7.0034\n",
      "Epoch 94/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 42.8633 - mae: 3.9831 - val_loss: 90.8504 - val_mae: 7.4578\n",
      "Epoch 95/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 44.1542 - mae: 4.1399 - val_loss: 81.0955 - val_mae: 6.5856\n",
      "Epoch 96/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 41.6519 - mae: 3.8991 - val_loss: 82.5694 - val_mae: 6.7979\n",
      "Epoch 97/200\n",
      "2526/2526 [==============================] - 1s 223us/step - loss: 42.3161 - mae: 4.0083 - val_loss: 82.7573 - val_mae: 6.5691\n",
      "Epoch 98/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 42.9516 - mae: 4.0501 - val_loss: 84.8096 - val_mae: 6.9515\n",
      "Epoch 99/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 42.0521 - mae: 3.9704 - val_loss: 81.8601 - val_mae: 6.6040\n",
      "Epoch 100/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 40.4436 - mae: 3.8802 - val_loss: 83.6433 - val_mae: 6.5666\n",
      "Epoch 101/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 40.4928 - mae: 3.8636 - val_loss: 87.2689 - val_mae: 7.1254\n",
      "Epoch 102/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 42.2997 - mae: 4.0476 - val_loss: 84.1257 - val_mae: 6.8424\n",
      "Epoch 103/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 41.8727 - mae: 4.0436 - val_loss: 86.5047 - val_mae: 6.5516\n",
      "Epoch 104/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 40.4784 - mae: 3.8996 - val_loss: 84.6176 - val_mae: 6.8900\n",
      "Epoch 105/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 41.9663 - mae: 4.0441 - val_loss: 88.1186 - val_mae: 6.9889\n",
      "Epoch 106/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 39.5985 - mae: 3.8024 - val_loss: 84.5516 - val_mae: 6.6164\n",
      "Epoch 107/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 41.4744 - mae: 4.0028 - val_loss: 85.5103 - val_mae: 6.7489\n",
      "Epoch 108/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 42.0948 - mae: 4.1074 - val_loss: 90.5936 - val_mae: 7.3721\n",
      "Epoch 109/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 40.5538 - mae: 4.0260 - val_loss: 91.5310 - val_mae: 7.3638\n",
      "Epoch 110/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 40.7603 - mae: 3.9791 - val_loss: 86.3676 - val_mae: 6.8202\n",
      "Epoch 111/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 39.2499 - mae: 3.8148 - val_loss: 85.7029 - val_mae: 6.8567\n",
      "Epoch 112/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 40.3298 - mae: 3.9509 - val_loss: 86.5880 - val_mae: 6.6133\n",
      "Epoch 113/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 39.4910 - mae: 3.8605 - val_loss: 89.7886 - val_mae: 6.6990\n",
      "Epoch 114/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 39.5693 - mae: 3.8453 - val_loss: 85.6382 - val_mae: 6.6904\n",
      "Epoch 115/200\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 38.9031 - mae: 3.8470 - val_loss: 87.1429 - val_mae: 6.7991\n",
      "Epoch 116/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 37.5120 - mae: 3.7628 - val_loss: 93.9669 - val_mae: 7.5390\n",
      "Epoch 117/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 40.2737 - mae: 4.0111 - val_loss: 89.1619 - val_mae: 6.8554\n",
      "Epoch 118/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 38.1746 - mae: 3.8125 - val_loss: 89.0281 - val_mae: 6.9111\n",
      "Epoch 119/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 38.2512 - mae: 3.7939 - val_loss: 91.5818 - val_mae: 7.2602\n",
      "Epoch 120/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 37.9953 - mae: 3.8155 - val_loss: 103.9203 - val_mae: 8.1043\n",
      "Epoch 121/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 39.1403 - mae: 3.9625 - val_loss: 89.2351 - val_mae: 7.0241\n",
      "Epoch 122/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 37.7804 - mae: 3.8230 - val_loss: 90.6033 - val_mae: 7.0543\n",
      "Epoch 123/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 36.9421 - mae: 3.6852 - val_loss: 90.5159 - val_mae: 7.0668\n",
      "Epoch 124/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 37.7832 - mae: 3.7914 - val_loss: 91.7174 - val_mae: 7.0608\n",
      "Epoch 125/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 37.0949 - mae: 3.7420 - val_loss: 94.2247 - val_mae: 7.4060\n",
      "Epoch 126/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 36.3060 - mae: 3.6789 - val_loss: 91.2247 - val_mae: 6.9865\n",
      "Epoch 127/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 36.4746 - mae: 3.7255 - val_loss: 92.8580 - val_mae: 7.0466\n",
      "Epoch 128/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 35.4014 - mae: 3.6489 - val_loss: 95.5740 - val_mae: 6.9720\n",
      "Epoch 129/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 35.7805 - mae: 3.6504 - val_loss: 95.0550 - val_mae: 7.3423\n",
      "Epoch 130/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 36.4741 - mae: 3.6781 - val_loss: 94.4421 - val_mae: 6.8831\n",
      "Epoch 131/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 34.2442 - mae: 3.5710 - val_loss: 94.1070 - val_mae: 7.2841\n",
      "Epoch 132/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 34.9456 - mae: 3.6091 - val_loss: 92.9407 - val_mae: 7.0811\n",
      "Epoch 133/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 34.8755 - mae: 3.5847 - val_loss: 95.1388 - val_mae: 7.0532\n",
      "Epoch 134/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 36.0966 - mae: 3.7263 - val_loss: 97.7175 - val_mae: 7.4098\n",
      "Epoch 135/200\n",
      "2526/2526 [==============================] - 0s 134us/step - loss: 36.4010 - mae: 3.7629 - val_loss: 92.7702 - val_mae: 7.1716\n",
      "Epoch 136/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 33.6688 - mae: 3.5269 - val_loss: 99.2488 - val_mae: 7.7146\n",
      "Epoch 137/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 33.6776 - mae: 3.4892 - val_loss: 94.2684 - val_mae: 7.1783\n",
      "Epoch 138/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 33.5202 - mae: 3.4574 - val_loss: 94.9276 - val_mae: 7.0539\n",
      "Epoch 139/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 34.2242 - mae: 3.5800 - val_loss: 97.2097 - val_mae: 7.4990\n",
      "Epoch 140/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 34.0565 - mae: 3.5712 - val_loss: 95.6333 - val_mae: 7.3229\n",
      "Epoch 141/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 33.9927 - mae: 3.4498 - val_loss: 94.9389 - val_mae: 7.3255\n",
      "Epoch 142/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 33.1605 - mae: 3.4940 - val_loss: 96.4862 - val_mae: 7.2847\n",
      "Epoch 143/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 34.6830 - mae: 3.6410 - val_loss: 101.4944 - val_mae: 7.1904\n",
      "Epoch 144/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 34.4262 - mae: 3.6298 - val_loss: 98.3655 - val_mae: 7.2379\n",
      "Epoch 145/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 32.6660 - mae: 3.4576 - val_loss: 97.1909 - val_mae: 7.2098\n",
      "Epoch 146/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 33.3154 - mae: 3.5433 - val_loss: 97.6714 - val_mae: 7.4173\n",
      "Epoch 147/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 32.1114 - mae: 3.4048 - val_loss: 98.7611 - val_mae: 7.5433\n",
      "Epoch 148/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 32.8012 - mae: 3.4763 - val_loss: 100.1399 - val_mae: 7.2622\n",
      "Epoch 149/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 33.2165 - mae: 3.5630 - val_loss: 99.0378 - val_mae: 7.3891\n",
      "Epoch 150/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 32.4288 - mae: 3.4918 - val_loss: 108.1121 - val_mae: 8.1247\n",
      "Epoch 151/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 33.3475 - mae: 3.5983 - val_loss: 100.4101 - val_mae: 7.3788\n",
      "Epoch 152/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 32.7838 - mae: 3.4273 - val_loss: 101.8259 - val_mae: 7.4936\n",
      "Epoch 153/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 32.4768 - mae: 3.5100 - val_loss: 101.0946 - val_mae: 7.3064\n",
      "Epoch 154/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 30.5792 - mae: 3.3333 - val_loss: 103.0722 - val_mae: 7.3793\n",
      "Epoch 155/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 32.3453 - mae: 3.5344 - val_loss: 100.8432 - val_mae: 7.4742\n",
      "Epoch 156/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 31.1162 - mae: 3.3816 - val_loss: 102.5183 - val_mae: 7.5075\n",
      "Epoch 157/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 31.2611 - mae: 3.3653 - val_loss: 101.4607 - val_mae: 7.5602\n",
      "Epoch 158/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 30.5700 - mae: 3.2975 - val_loss: 105.3399 - val_mae: 7.6086\n",
      "Epoch 159/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 31.5466 - mae: 3.4630 - val_loss: 102.9649 - val_mae: 7.6733\n",
      "Epoch 160/200\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 30.6274 - mae: 3.3714 - val_loss: 110.8336 - val_mae: 8.2340\n",
      "Epoch 161/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 30.2495 - mae: 3.4193 - val_loss: 105.1546 - val_mae: 7.8008\n",
      "Epoch 162/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 30.2548 - mae: 3.2981 - val_loss: 103.5302 - val_mae: 7.5799\n",
      "Epoch 163/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 30.2043 - mae: 3.3096 - val_loss: 105.7496 - val_mae: 7.5267\n",
      "Epoch 164/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 31.3781 - mae: 3.5310 - val_loss: 105.1864 - val_mae: 7.6350\n",
      "Epoch 165/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 29.9383 - mae: 3.3488 - val_loss: 115.6947 - val_mae: 7.8147\n",
      "Epoch 166/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 32.1947 - mae: 3.6303 - val_loss: 104.6430 - val_mae: 7.5661\n",
      "Epoch 167/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 29.4528 - mae: 3.2781 - val_loss: 111.7340 - val_mae: 8.1177\n",
      "Epoch 168/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 30.8363 - mae: 3.4753 - val_loss: 105.0718 - val_mae: 7.5823\n",
      "Epoch 169/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 28.7068 - mae: 3.2217 - val_loss: 107.5298 - val_mae: 7.6916\n",
      "Epoch 170/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 29.2060 - mae: 3.2964 - val_loss: 108.0217 - val_mae: 7.8704\n",
      "Epoch 171/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 28.1434 - mae: 3.1395 - val_loss: 107.3182 - val_mae: 7.7706\n",
      "Epoch 172/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 29.5210 - mae: 3.3685 - val_loss: 113.9526 - val_mae: 8.3173\n",
      "Epoch 173/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 29.6097 - mae: 3.3451 - val_loss: 111.2045 - val_mae: 7.9824\n",
      "Epoch 174/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 29.1733 - mae: 3.2879 - val_loss: 107.5504 - val_mae: 7.7602\n",
      "Epoch 175/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 27.9580 - mae: 3.1726 - val_loss: 109.2071 - val_mae: 7.6288\n",
      "Epoch 176/200\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 30.7318 - mae: 3.5072 - val_loss: 109.4476 - val_mae: 7.6327\n",
      "Epoch 177/200\n",
      "2526/2526 [==============================] - 0s 175us/step - loss: 29.6117 - mae: 3.3396 - val_loss: 107.5989 - val_mae: 7.6359\n",
      "Epoch 178/200\n",
      "2526/2526 [==============================] - 0s 157us/step - loss: 29.2006 - mae: 3.3787 - val_loss: 113.7010 - val_mae: 8.2823\n",
      "Epoch 179/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 27.0520 - mae: 3.1368 - val_loss: 111.7465 - val_mae: 7.9465\n",
      "Epoch 180/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 27.6990 - mae: 3.2124 - val_loss: 109.3136 - val_mae: 7.8893\n",
      "Epoch 181/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 27.1047 - mae: 3.1057 - val_loss: 111.8535 - val_mae: 8.0297\n",
      "Epoch 182/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 27.5279 - mae: 3.1488 - val_loss: 111.1517 - val_mae: 8.0055\n",
      "Epoch 183/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 26.9775 - mae: 3.1280 - val_loss: 112.0979 - val_mae: 7.9210\n",
      "Epoch 184/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 27.0872 - mae: 3.1273 - val_loss: 111.0697 - val_mae: 7.9189\n",
      "Epoch 185/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 27.3868 - mae: 3.2174 - val_loss: 111.2878 - val_mae: 7.9542\n",
      "Epoch 186/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 27.0503 - mae: 3.2039 - val_loss: 113.8985 - val_mae: 7.9070\n",
      "Epoch 187/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 27.1690 - mae: 3.2257 - val_loss: 114.5361 - val_mae: 7.8800\n",
      "Epoch 188/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 26.9260 - mae: 3.1302 - val_loss: 112.4257 - val_mae: 7.8581\n",
      "Epoch 189/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 27.7465 - mae: 3.2593 - val_loss: 113.9197 - val_mae: 8.1312\n",
      "Epoch 190/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 27.0431 - mae: 3.1690 - val_loss: 117.7905 - val_mae: 8.3804\n",
      "Epoch 191/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 26.6663 - mae: 3.1882 - val_loss: 113.7961 - val_mae: 7.9433\n",
      "Epoch 192/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 26.3566 - mae: 3.1123 - val_loss: 115.9619 - val_mae: 8.2116\n",
      "Epoch 193/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 25.8023 - mae: 3.0220 - val_loss: 115.6344 - val_mae: 8.0582\n",
      "Epoch 194/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 29.3776 - mae: 3.5208 - val_loss: 117.9334 - val_mae: 8.3515\n",
      "Epoch 195/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 27.2536 - mae: 3.2316 - val_loss: 114.8812 - val_mae: 7.8990\n",
      "Epoch 196/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 28.6264 - mae: 3.3552 - val_loss: 118.3825 - val_mae: 8.3876\n",
      "Epoch 197/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 26.5037 - mae: 3.1453 - val_loss: 126.7669 - val_mae: 8.8459\n",
      "Epoch 198/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 26.5419 - mae: 3.1597 - val_loss: 115.8368 - val_mae: 8.0061\n",
      "Epoch 199/200\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 26.7398 - mae: 3.1912 - val_loss: 114.8876 - val_mae: 7.8705\n",
      "Epoch 200/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 25.8931 - mae: 3.0856 - val_loss: 112.8986 - val_mae: 7.9368\n",
      "processing fold # 3\n",
      "Train on 2526 samples, validate on 842 samples\n",
      "Epoch 1/200\n",
      "2526/2526 [==============================] - 1s 200us/step - loss: 4470.2734 - mae: 55.5379 - val_loss: 244.1312 - val_mae: 12.0773\n",
      "Epoch 2/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 201.6000 - mae: 10.4533 - val_loss: 143.0407 - val_mae: 8.7276\n",
      "Epoch 3/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 123.9748 - mae: 7.7874 - val_loss: 108.6576 - val_mae: 7.1112\n",
      "Epoch 4/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 98.3475 - mae: 6.6454 - val_loss: 90.4474 - val_mae: 6.4130\n",
      "Epoch 5/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 89.8032 - mae: 6.1627 - val_loss: 87.6861 - val_mae: 6.0338\n",
      "Epoch 6/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 84.4085 - mae: 5.8681 - val_loss: 81.4947 - val_mae: 6.1256\n",
      "Epoch 7/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 81.9423 - mae: 5.8160 - val_loss: 80.5450 - val_mae: 6.2166\n",
      "Epoch 8/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 79.6748 - mae: 5.6547 - val_loss: 79.8795 - val_mae: 6.2258\n",
      "Epoch 9/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 79.1203 - mae: 5.6666 - val_loss: 80.8539 - val_mae: 5.7822\n",
      "Epoch 10/200\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 76.5691 - mae: 5.5348 - val_loss: 78.4069 - val_mae: 6.1600\n",
      "Epoch 11/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 76.1462 - mae: 5.5009 - val_loss: 78.5907 - val_mae: 5.8637\n",
      "Epoch 12/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 75.2835 - mae: 5.4353 - val_loss: 78.5823 - val_mae: 6.2926\n",
      "Epoch 13/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 74.2087 - mae: 5.4463 - val_loss: 78.6761 - val_mae: 5.8717\n",
      "Epoch 14/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 74.5979 - mae: 5.4865 - val_loss: 77.9712 - val_mae: 6.1492\n",
      "Epoch 15/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 73.2183 - mae: 5.3981 - val_loss: 78.6246 - val_mae: 5.8903\n",
      "Epoch 16/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 72.2693 - mae: 5.3401 - val_loss: 77.8978 - val_mae: 6.1231\n",
      "Epoch 17/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 71.6667 - mae: 5.3232 - val_loss: 81.1274 - val_mae: 6.6762\n",
      "Epoch 18/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 72.0203 - mae: 5.3870 - val_loss: 79.1690 - val_mae: 5.7689\n",
      "Epoch 19/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 70.3660 - mae: 5.2607 - val_loss: 79.6846 - val_mae: 6.5720\n",
      "Epoch 20/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 69.8677 - mae: 5.3097 - val_loss: 78.5578 - val_mae: 5.8411\n",
      "Epoch 21/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 69.4224 - mae: 5.2545 - val_loss: 77.9146 - val_mae: 6.0738\n",
      "Epoch 22/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 68.9762 - mae: 5.1963 - val_loss: 78.0126 - val_mae: 6.0331\n",
      "Epoch 23/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 68.3129 - mae: 5.2135 - val_loss: 86.0574 - val_mae: 7.1849\n",
      "Epoch 24/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 68.2397 - mae: 5.2097 - val_loss: 82.1336 - val_mae: 6.7587\n",
      "Epoch 25/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 67.3651 - mae: 5.2398 - val_loss: 78.9710 - val_mae: 6.0451\n",
      "Epoch 26/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 65.9989 - mae: 5.0970 - val_loss: 79.3487 - val_mae: 6.2689\n",
      "Epoch 27/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 65.7963 - mae: 5.1093 - val_loss: 78.5795 - val_mae: 5.9855\n",
      "Epoch 28/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 65.4568 - mae: 5.0785 - val_loss: 82.1010 - val_mae: 6.7279\n",
      "Epoch 29/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 65.5749 - mae: 5.1444 - val_loss: 80.8571 - val_mae: 5.9604\n",
      "Epoch 30/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 63.5251 - mae: 5.0357 - val_loss: 79.2701 - val_mae: 6.0656\n",
      "Epoch 31/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 63.9241 - mae: 5.1196 - val_loss: 79.4667 - val_mae: 6.0701\n",
      "Epoch 32/200\n",
      "2526/2526 [==============================] - 0s 153us/step - loss: 63.1839 - mae: 5.0257 - val_loss: 79.3735 - val_mae: 6.0619\n",
      "Epoch 33/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 61.4431 - mae: 4.9562 - val_loss: 80.5287 - val_mae: 6.2856\n",
      "Epoch 34/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 62.3376 - mae: 5.0448 - val_loss: 80.3700 - val_mae: 5.9367\n",
      "Epoch 35/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 60.6945 - mae: 4.8941 - val_loss: 81.7162 - val_mae: 6.4154\n",
      "Epoch 36/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 61.3630 - mae: 4.9889 - val_loss: 84.6136 - val_mae: 5.9489\n",
      "Epoch 37/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 60.7880 - mae: 4.9707 - val_loss: 82.4146 - val_mae: 6.5335\n",
      "Epoch 38/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 60.4816 - mae: 4.9247 - val_loss: 83.7210 - val_mae: 5.9449\n",
      "Epoch 39/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 59.8466 - mae: 4.8744 - val_loss: 85.6075 - val_mae: 6.0226\n",
      "Epoch 40/200\n",
      "2526/2526 [==============================] - 0s 155us/step - loss: 57.9035 - mae: 4.7482 - val_loss: 87.0989 - val_mae: 6.8865\n",
      "Epoch 41/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 57.8203 - mae: 4.8533 - val_loss: 85.9757 - val_mae: 6.6503\n",
      "Epoch 42/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 56.3605 - mae: 4.7363 - val_loss: 85.7170 - val_mae: 6.1276\n",
      "Epoch 43/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 57.2711 - mae: 4.7906 - val_loss: 87.2665 - val_mae: 6.0280\n",
      "Epoch 44/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 56.3720 - mae: 4.7628 - val_loss: 86.5333 - val_mae: 6.5009\n",
      "Epoch 45/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 55.7064 - mae: 4.6516 - val_loss: 86.9940 - val_mae: 6.4188\n",
      "Epoch 46/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 54.8894 - mae: 4.6783 - val_loss: 87.7523 - val_mae: 6.2324\n",
      "Epoch 47/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 53.6121 - mae: 4.6014 - val_loss: 87.7613 - val_mae: 6.4345\n",
      "Epoch 48/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 53.3438 - mae: 4.5656 - val_loss: 90.7108 - val_mae: 6.5412\n",
      "Epoch 49/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 55.5870 - mae: 4.7854 - val_loss: 90.8165 - val_mae: 6.8588\n",
      "Epoch 50/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 52.2033 - mae: 4.5427 - val_loss: 89.4473 - val_mae: 6.3311\n",
      "Epoch 51/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 50.7633 - mae: 4.4603 - val_loss: 94.4828 - val_mae: 6.3503\n",
      "Epoch 52/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 51.5335 - mae: 4.5069 - val_loss: 89.7477 - val_mae: 6.6429\n",
      "Epoch 53/200\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 51.0373 - mae: 4.5134 - val_loss: 94.2389 - val_mae: 6.3565\n",
      "Epoch 54/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 51.5634 - mae: 4.5977 - val_loss: 91.7807 - val_mae: 6.8790\n",
      "Epoch 55/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 50.3412 - mae: 4.4715 - val_loss: 103.0524 - val_mae: 7.7243\n",
      "Epoch 56/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 50.6744 - mae: 4.5310 - val_loss: 91.3215 - val_mae: 6.5686\n",
      "Epoch 57/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 48.5555 - mae: 4.4076 - val_loss: 92.3336 - val_mae: 6.5463\n",
      "Epoch 58/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 46.8526 - mae: 4.2658 - val_loss: 93.9365 - val_mae: 6.8758\n",
      "Epoch 59/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 47.3229 - mae: 4.3348 - val_loss: 94.3611 - val_mae: 6.8921\n",
      "Epoch 60/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 47.1977 - mae: 4.3256 - val_loss: 93.6038 - val_mae: 6.6875\n",
      "Epoch 61/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 46.3956 - mae: 4.2765 - val_loss: 97.4940 - val_mae: 7.1429\n",
      "Epoch 62/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 45.9611 - mae: 4.2346 - val_loss: 95.4628 - val_mae: 6.9215\n",
      "Epoch 63/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 45.6445 - mae: 4.2808 - val_loss: 94.8564 - val_mae: 6.6883\n",
      "Epoch 64/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 44.3573 - mae: 4.1677 - val_loss: 99.8395 - val_mae: 6.7183\n",
      "Epoch 65/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 44.4483 - mae: 4.1570 - val_loss: 102.4343 - val_mae: 6.8062\n",
      "Epoch 66/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 44.1890 - mae: 4.1715 - val_loss: 102.0141 - val_mae: 6.8878\n",
      "Epoch 67/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 43.0274 - mae: 4.0693 - val_loss: 100.2930 - val_mae: 6.7392\n",
      "Epoch 68/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 43.5864 - mae: 4.1061 - val_loss: 102.5887 - val_mae: 7.4126\n",
      "Epoch 69/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 42.2982 - mae: 4.1138 - val_loss: 99.4181 - val_mae: 6.8969\n",
      "Epoch 70/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 42.7984 - mae: 4.1163 - val_loss: 100.4405 - val_mae: 7.0235\n",
      "Epoch 71/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 41.4303 - mae: 4.0005 - val_loss: 102.6454 - val_mae: 7.0291\n",
      "Epoch 72/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 43.0501 - mae: 4.1681 - val_loss: 103.4561 - val_mae: 6.9482\n",
      "Epoch 73/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 42.3956 - mae: 4.0610 - val_loss: 106.0734 - val_mae: 7.4255\n",
      "Epoch 74/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 41.5604 - mae: 4.0680 - val_loss: 109.8289 - val_mae: 7.0531\n",
      "Epoch 75/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 39.7134 - mae: 3.8907 - val_loss: 104.6215 - val_mae: 7.1788\n",
      "Epoch 76/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 39.6410 - mae: 3.8812 - val_loss: 104.0946 - val_mae: 7.1726\n",
      "Epoch 77/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 39.4975 - mae: 3.8946 - val_loss: 107.4953 - val_mae: 7.2242\n",
      "Epoch 78/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 39.9828 - mae: 3.9368 - val_loss: 115.5774 - val_mae: 8.0794\n",
      "Epoch 79/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 39.9676 - mae: 4.0204 - val_loss: 106.9150 - val_mae: 7.1932\n",
      "Epoch 80/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 38.7814 - mae: 3.9014 - val_loss: 110.9108 - val_mae: 7.6227\n",
      "Epoch 81/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 38.1808 - mae: 3.8058 - val_loss: 110.2022 - val_mae: 7.3052\n",
      "Epoch 82/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 39.6163 - mae: 4.0070 - val_loss: 110.1675 - val_mae: 7.2895\n",
      "Epoch 83/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 37.5093 - mae: 3.8065 - val_loss: 106.2499 - val_mae: 7.2959\n",
      "Epoch 84/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 36.5067 - mae: 3.7471 - val_loss: 111.8752 - val_mae: 7.4609\n",
      "Epoch 85/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 37.1895 - mae: 3.7803 - val_loss: 119.7885 - val_mae: 7.5444\n",
      "Epoch 86/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 37.2611 - mae: 3.8452 - val_loss: 112.8737 - val_mae: 7.4636\n",
      "Epoch 87/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 35.5727 - mae: 3.6712 - val_loss: 116.1412 - val_mae: 7.4155\n",
      "Epoch 88/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 35.7712 - mae: 3.7281 - val_loss: 116.9454 - val_mae: 7.9043\n",
      "Epoch 89/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 35.9232 - mae: 3.6864 - val_loss: 113.1879 - val_mae: 7.6476\n",
      "Epoch 90/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 35.6195 - mae: 3.6658 - val_loss: 120.6580 - val_mae: 8.1872\n",
      "Epoch 91/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 35.5261 - mae: 3.7353 - val_loss: 115.4799 - val_mae: 7.6895\n",
      "Epoch 92/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 35.3217 - mae: 3.7258 - val_loss: 118.5222 - val_mae: 7.9022\n",
      "Epoch 93/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 34.5008 - mae: 3.6866 - val_loss: 115.1876 - val_mae: 7.6275\n",
      "Epoch 94/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 35.4286 - mae: 3.7369 - val_loss: 114.8689 - val_mae: 7.6040\n",
      "Epoch 95/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 34.5458 - mae: 3.7034 - val_loss: 119.3214 - val_mae: 7.5743\n",
      "Epoch 96/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 34.2726 - mae: 3.6781 - val_loss: 120.0320 - val_mae: 7.6639\n",
      "Epoch 97/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 33.6943 - mae: 3.6474 - val_loss: 118.0405 - val_mae: 7.6366\n",
      "Epoch 98/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 32.4695 - mae: 3.5228 - val_loss: 119.3665 - val_mae: 7.6861\n",
      "Epoch 99/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 33.1680 - mae: 3.5691 - val_loss: 122.5273 - val_mae: 7.9413\n",
      "Epoch 100/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 33.3236 - mae: 3.5777 - val_loss: 120.3164 - val_mae: 7.8923\n",
      "Epoch 101/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 34.2821 - mae: 3.7221 - val_loss: 118.0155 - val_mae: 7.7406\n",
      "Epoch 102/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 32.3888 - mae: 3.5636 - val_loss: 119.3988 - val_mae: 7.8362\n",
      "Epoch 103/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 32.2080 - mae: 3.4857 - val_loss: 124.7346 - val_mae: 7.9326\n",
      "Epoch 104/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 30.9315 - mae: 3.3980 - val_loss: 123.5401 - val_mae: 8.1620\n",
      "Epoch 105/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 32.3590 - mae: 3.6108 - val_loss: 122.3417 - val_mae: 7.8700\n",
      "Epoch 106/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 31.1970 - mae: 3.4328 - val_loss: 121.2558 - val_mae: 7.8922\n",
      "Epoch 107/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 29.8591 - mae: 3.3124 - val_loss: 123.8456 - val_mae: 7.8704\n",
      "Epoch 108/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 30.6954 - mae: 3.4571 - val_loss: 123.1363 - val_mae: 7.9180\n",
      "Epoch 109/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 31.1089 - mae: 3.4509 - val_loss: 126.1883 - val_mae: 8.2407\n",
      "Epoch 110/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 32.2393 - mae: 3.5973 - val_loss: 119.1354 - val_mae: 7.7964\n",
      "Epoch 111/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 29.9605 - mae: 3.4383 - val_loss: 126.2484 - val_mae: 8.1186\n",
      "Epoch 112/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 31.0315 - mae: 3.5172 - val_loss: 127.7903 - val_mae: 8.0078\n",
      "Epoch 113/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 29.4760 - mae: 3.3738 - val_loss: 122.5589 - val_mae: 7.8683\n",
      "Epoch 114/200\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 29.8647 - mae: 3.3684 - val_loss: 126.4282 - val_mae: 8.0602\n",
      "Epoch 115/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 30.1254 - mae: 3.4991 - val_loss: 127.5502 - val_mae: 8.2229\n",
      "Epoch 116/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 28.6555 - mae: 3.2607 - val_loss: 128.8967 - val_mae: 8.2257\n",
      "Epoch 117/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 28.7549 - mae: 3.3052 - val_loss: 130.8890 - val_mae: 8.3411\n",
      "Epoch 118/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 28.7336 - mae: 3.3208 - val_loss: 125.9722 - val_mae: 8.1487\n",
      "Epoch 119/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 28.2487 - mae: 3.3169 - val_loss: 126.4243 - val_mae: 8.0691\n",
      "Epoch 120/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 28.4608 - mae: 3.2948 - val_loss: 127.4535 - val_mae: 8.0126\n",
      "Epoch 121/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 28.1898 - mae: 3.2550 - val_loss: 130.7892 - val_mae: 8.0466\n",
      "Epoch 122/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 29.2358 - mae: 3.4123 - val_loss: 128.4829 - val_mae: 8.0989\n",
      "Epoch 123/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 27.5787 - mae: 3.2577 - val_loss: 128.3178 - val_mae: 8.0433\n",
      "Epoch 124/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 27.7165 - mae: 3.2763 - val_loss: 131.1007 - val_mae: 8.0455\n",
      "Epoch 125/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 27.2862 - mae: 3.2646 - val_loss: 129.5355 - val_mae: 8.1675\n",
      "Epoch 126/200\n",
      "2526/2526 [==============================] - 0s 148us/step - loss: 27.0710 - mae: 3.2159 - val_loss: 128.1995 - val_mae: 8.0589\n",
      "Epoch 127/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 26.7746 - mae: 3.1899 - val_loss: 131.0498 - val_mae: 8.1654\n",
      "Epoch 128/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 26.8926 - mae: 3.2092 - val_loss: 133.4510 - val_mae: 8.2538\n",
      "Epoch 129/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 27.4287 - mae: 3.2817 - val_loss: 137.0229 - val_mae: 8.3406\n",
      "Epoch 130/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 26.6364 - mae: 3.2967 - val_loss: 131.3646 - val_mae: 8.3879\n",
      "Epoch 131/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 27.5779 - mae: 3.3809 - val_loss: 131.8297 - val_mae: 8.1709\n",
      "Epoch 132/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 26.3423 - mae: 3.1709 - val_loss: 133.1254 - val_mae: 8.3645\n",
      "Epoch 133/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 25.4316 - mae: 3.0918 - val_loss: 136.0772 - val_mae: 8.2927\n",
      "Epoch 134/200\n",
      "2526/2526 [==============================] - 0s 147us/step - loss: 24.8876 - mae: 3.0689 - val_loss: 129.9184 - val_mae: 8.1258\n",
      "Epoch 135/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 26.0641 - mae: 3.1546 - val_loss: 134.7273 - val_mae: 8.3962\n",
      "Epoch 136/200\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 26.1514 - mae: 3.2121 - val_loss: 142.1183 - val_mae: 8.8617\n",
      "Epoch 137/200\n",
      "2526/2526 [==============================] - 0s 172us/step - loss: 24.9828 - mae: 3.0652 - val_loss: 131.5183 - val_mae: 8.1479\n",
      "Epoch 138/200\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 24.7001 - mae: 3.1081 - val_loss: 137.5063 - val_mae: 8.2856\n",
      "Epoch 139/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 25.3614 - mae: 3.2321 - val_loss: 132.3869 - val_mae: 8.2196\n",
      "Epoch 140/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 24.6641 - mae: 3.0746 - val_loss: 139.0781 - val_mae: 8.3510\n",
      "Epoch 141/200\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 24.8128 - mae: 3.1883 - val_loss: 134.8280 - val_mae: 8.3462\n",
      "Epoch 142/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 25.1521 - mae: 3.1561 - val_loss: 148.0061 - val_mae: 8.9835\n",
      "Epoch 143/200\n",
      "2526/2526 [==============================] - 0s 158us/step - loss: 25.2079 - mae: 3.1902 - val_loss: 138.7958 - val_mae: 8.4674\n",
      "Epoch 144/200\n",
      "2526/2526 [==============================] - 0s 164us/step - loss: 23.5533 - mae: 3.0520 - val_loss: 138.2354 - val_mae: 8.3439\n",
      "Epoch 145/200\n",
      "2526/2526 [==============================] - 0s 164us/step - loss: 23.9828 - mae: 3.0848 - val_loss: 135.8476 - val_mae: 8.2657\n",
      "Epoch 146/200\n",
      "2526/2526 [==============================] - 0s 161us/step - loss: 24.3690 - mae: 3.1300 - val_loss: 135.0907 - val_mae: 8.3438\n",
      "Epoch 147/200\n",
      "2526/2526 [==============================] - 0s 160us/step - loss: 24.8715 - mae: 3.1706 - val_loss: 137.9483 - val_mae: 8.2763\n",
      "Epoch 148/200\n",
      "2526/2526 [==============================] - 0s 165us/step - loss: 23.8465 - mae: 3.0697 - val_loss: 138.6388 - val_mae: 8.4969\n",
      "Epoch 149/200\n",
      "2526/2526 [==============================] - 0s 162us/step - loss: 23.6899 - mae: 3.1242 - val_loss: 137.4049 - val_mae: 8.2802\n",
      "Epoch 150/200\n",
      "2526/2526 [==============================] - 0s 177us/step - loss: 22.8753 - mae: 2.9548 - val_loss: 137.0453 - val_mae: 8.3069\n",
      "Epoch 151/200\n",
      "2526/2526 [==============================] - 0s 180us/step - loss: 22.9600 - mae: 3.0273 - val_loss: 136.9197 - val_mae: 8.4307\n",
      "Epoch 152/200\n",
      "2526/2526 [==============================] - 0s 155us/step - loss: 22.6637 - mae: 2.9601 - val_loss: 140.3975 - val_mae: 8.5807\n",
      "Epoch 153/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 25.1134 - mae: 3.2686 - val_loss: 145.6862 - val_mae: 8.5953\n",
      "Epoch 154/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 22.7109 - mae: 2.9632 - val_loss: 144.8907 - val_mae: 8.6953\n",
      "Epoch 155/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 22.0851 - mae: 2.9572 - val_loss: 139.0065 - val_mae: 8.5332\n",
      "Epoch 156/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 22.8710 - mae: 3.0393 - val_loss: 144.5923 - val_mae: 8.8002\n",
      "Epoch 157/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 23.6035 - mae: 3.1757 - val_loss: 142.7467 - val_mae: 8.5471\n",
      "Epoch 158/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 22.4374 - mae: 3.0424 - val_loss: 141.9066 - val_mae: 8.5357\n",
      "Epoch 159/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 21.6786 - mae: 2.9380 - val_loss: 145.4456 - val_mae: 8.7099\n",
      "Epoch 160/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 23.0010 - mae: 3.0576 - val_loss: 146.5952 - val_mae: 8.5619\n",
      "Epoch 161/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 22.1014 - mae: 3.0312 - val_loss: 143.6435 - val_mae: 8.5835\n",
      "Epoch 162/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 21.9456 - mae: 3.0438 - val_loss: 144.6371 - val_mae: 8.5606\n",
      "Epoch 163/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 21.8208 - mae: 2.9608 - val_loss: 142.1376 - val_mae: 8.4643\n",
      "Epoch 164/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 21.6527 - mae: 2.9265 - val_loss: 142.8061 - val_mae: 8.5697\n",
      "Epoch 165/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 20.9102 - mae: 2.8820 - val_loss: 149.0835 - val_mae: 8.6713\n",
      "Epoch 166/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 22.2090 - mae: 3.1120 - val_loss: 145.2195 - val_mae: 8.6859\n",
      "Epoch 167/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 21.1700 - mae: 2.9125 - val_loss: 148.1985 - val_mae: 8.7282\n",
      "Epoch 168/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 19.8697 - mae: 2.7463 - val_loss: 148.0522 - val_mae: 8.7130\n",
      "Epoch 169/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 20.8839 - mae: 2.8909 - val_loss: 149.3852 - val_mae: 8.7817\n",
      "Epoch 170/200\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 21.5315 - mae: 2.9481 - val_loss: 147.5077 - val_mae: 8.6702\n",
      "Epoch 171/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 20.6342 - mae: 2.9138 - val_loss: 151.1000 - val_mae: 8.7109\n",
      "Epoch 172/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 20.7508 - mae: 2.9772 - val_loss: 148.0133 - val_mae: 8.7624\n",
      "Epoch 173/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 19.5445 - mae: 2.7450 - val_loss: 146.9980 - val_mae: 8.5978\n",
      "Epoch 174/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 19.3791 - mae: 2.7115 - val_loss: 155.7165 - val_mae: 8.8786\n",
      "Epoch 175/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 20.9885 - mae: 2.9505 - val_loss: 145.2879 - val_mae: 8.5533\n",
      "Epoch 176/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 20.0517 - mae: 2.8731 - val_loss: 149.6295 - val_mae: 8.7923\n",
      "Epoch 177/200\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 19.9366 - mae: 2.8046 - val_loss: 144.7110 - val_mae: 8.5567\n",
      "Epoch 178/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 19.5941 - mae: 2.7991 - val_loss: 149.3140 - val_mae: 8.6969\n",
      "Epoch 179/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 19.3770 - mae: 2.7732 - val_loss: 144.5787 - val_mae: 8.4961\n",
      "Epoch 180/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 19.2856 - mae: 2.7617 - val_loss: 144.2679 - val_mae: 8.5929\n",
      "Epoch 181/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 19.1146 - mae: 2.7900 - val_loss: 150.7052 - val_mae: 8.6939\n",
      "Epoch 182/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 20.0831 - mae: 2.8444 - val_loss: 147.8129 - val_mae: 8.7398\n",
      "Epoch 183/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 18.7949 - mae: 2.7130 - val_loss: 146.3967 - val_mae: 8.6448\n",
      "Epoch 184/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 21.3909 - mae: 3.0616 - val_loss: 148.4256 - val_mae: 8.6557\n",
      "Epoch 185/200\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 19.2998 - mae: 2.8045 - val_loss: 145.7778 - val_mae: 8.5515\n",
      "Epoch 186/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 18.3520 - mae: 2.6750 - val_loss: 157.7044 - val_mae: 8.8857\n",
      "Epoch 187/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 18.7254 - mae: 2.6996 - val_loss: 160.2531 - val_mae: 8.9281\n",
      "Epoch 188/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 19.6769 - mae: 2.9330 - val_loss: 152.3776 - val_mae: 8.7642\n",
      "Epoch 189/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 19.1139 - mae: 2.7572 - val_loss: 149.7890 - val_mae: 8.7237\n",
      "Epoch 190/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 18.4637 - mae: 2.6829 - val_loss: 149.7892 - val_mae: 8.7774\n",
      "Epoch 191/200\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 17.3253 - mae: 2.6138 - val_loss: 153.3993 - val_mae: 8.7808\n",
      "Epoch 192/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 18.0015 - mae: 2.7160 - val_loss: 155.9642 - val_mae: 9.1448\n",
      "Epoch 193/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 20.7044 - mae: 3.0430 - val_loss: 153.5853 - val_mae: 8.7777\n",
      "Epoch 194/200\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 18.6949 - mae: 2.8195 - val_loss: 155.8773 - val_mae: 8.8600\n",
      "Epoch 195/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 18.7339 - mae: 2.8145 - val_loss: 155.5388 - val_mae: 8.8428\n",
      "Epoch 196/200\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 18.4221 - mae: 2.7366 - val_loss: 153.0726 - val_mae: 8.7283\n",
      "Epoch 197/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 17.9544 - mae: 2.6464 - val_loss: 150.6279 - val_mae: 8.6755\n",
      "Epoch 198/200\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 18.5201 - mae: 2.7696 - val_loss: 155.8297 - val_mae: 8.9654\n",
      "Epoch 199/200\n",
      "2526/2526 [==============================] - 0s 143us/step - loss: 17.8074 - mae: 2.6990 - val_loss: 158.5054 - val_mae: 9.0558\n",
      "Epoch 200/200\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 17.4288 - mae: 2.5949 - val_loss: 154.9567 - val_mae: 8.9171\n"
     ]
    }
   ],
   "source": [
    "all_mae_histories = []\n",
    "\n",
    "for i in range(k):\n",
    "    #1\n",
    "    print('processing fold #', i)\n",
    "    #2\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    #3\n",
    "    val_labels = train_labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    #4\n",
    "    partial_train_data = pd.concat(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    #5\n",
    "    partial_train_labels = pd.concat(\n",
    "        [train_labels[:i * num_val_samples],\n",
    "         train_labels[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    #6\n",
    "    model = build_model()\n",
    "    #7\n",
    "    history = model.fit(partial_train_data, partial_train_labels,\n",
    "                        validation_data=(val_data, val_labels),\n",
    "                        epochs=num_epochs, batch_size=32, verbose=1)\n",
    "    #8\n",
    "    mae_history = history.history['val_mae']\n",
    "    #9\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem Training wird zur Visualisierung zunächst der mittlere MAE je Epoche gebildet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.473861455917358,\n",
       " 9.044975280761719,\n",
       " 7.487056493759155,\n",
       " 6.646254897117615,\n",
       " 6.221865773200989,\n",
       " 6.039333820343018,\n",
       " 6.048254609107971,\n",
       " 5.941379427909851,\n",
       " 5.921663165092468,\n",
       " 6.052186489105225,\n",
       " 5.843148827552795,\n",
       " 5.929646730422974,\n",
       " 5.992798209190369,\n",
       " 6.039876699447632,\n",
       " 5.8804813623428345,\n",
       " 5.906570792198181,\n",
       " 6.191679120063782,\n",
       " 5.887655735015869,\n",
       " 6.142213702201843,\n",
       " 5.750662207603455,\n",
       " 5.72569739818573,\n",
       " 6.250012516975403,\n",
       " 6.12990140914917,\n",
       " 5.959758758544922,\n",
       " 5.934161901473999,\n",
       " 5.910230398178101,\n",
       " 5.750515460968018,\n",
       " 6.332314610481262,\n",
       " 6.320896029472351,\n",
       " 5.931284070014954,\n",
       " 5.9908270835876465,\n",
       " 5.901573657989502,\n",
       " 6.013461232185364,\n",
       " 6.103154182434082,\n",
       " 6.3550403118133545,\n",
       " 6.248668313026428,\n",
       " 6.3532350063323975,\n",
       " 5.893381834030151,\n",
       " 6.411098480224609,\n",
       " 6.437928915023804,\n",
       " 6.1024510860443115,\n",
       " 6.259112596511841,\n",
       " 5.957106113433838,\n",
       " 6.077626943588257,\n",
       " 6.194869756698608,\n",
       " 6.50044310092926,\n",
       " 6.449777841567993,\n",
       " 6.089330315589905,\n",
       " 6.67515504360199,\n",
       " 6.273118853569031,\n",
       " 6.3090500831604,\n",
       " 6.429985404014587,\n",
       " 6.818521499633789,\n",
       " 6.388335824012756,\n",
       " 6.617078185081482,\n",
       " 6.520488142967224,\n",
       " 6.414792418479919,\n",
       " 6.496027946472168,\n",
       " 6.777924656867981,\n",
       " 6.600887298583984,\n",
       " 6.615785241127014,\n",
       " 6.595040917396545,\n",
       " 6.49292778968811,\n",
       " 6.4874507188797,\n",
       " 6.628871560096741,\n",
       " 6.946443200111389,\n",
       " 6.738733172416687,\n",
       " 6.619116425514221,\n",
       " 6.801064729690552,\n",
       " 6.753337025642395,\n",
       " 6.85788369178772,\n",
       " 6.878326773643494,\n",
       " 6.951617479324341,\n",
       " 6.812949895858765,\n",
       " 6.857646584510803,\n",
       " 6.635373592376709,\n",
       " 6.742125511169434,\n",
       " 7.051005125045776,\n",
       " 6.953025221824646,\n",
       " 6.951130509376526,\n",
       " 6.965235948562622,\n",
       " 6.862034797668457,\n",
       " 7.1902174949646,\n",
       " 6.9942357540130615,\n",
       " 6.860087156295776,\n",
       " 7.359516263008118,\n",
       " 6.831874847412109,\n",
       " 7.277623534202576,\n",
       " 7.137613654136658,\n",
       " 7.175927996635437,\n",
       " 7.228973984718323,\n",
       " 7.084930181503296,\n",
       " 7.106731653213501,\n",
       " 7.25956928730011,\n",
       " 7.112585663795471,\n",
       " 7.09971022605896,\n",
       " 7.247231960296631,\n",
       " 7.300476431846619,\n",
       " 7.19845449924469,\n",
       " 7.139486312866211,\n",
       " 7.320250034332275,\n",
       " 7.294037938117981,\n",
       " 7.232089042663574,\n",
       " 7.402218341827393,\n",
       " 7.351596236228943,\n",
       " 7.221071720123291,\n",
       " 7.201281666755676,\n",
       " 7.616867423057556,\n",
       " 7.505731105804443,\n",
       " 7.218159198760986,\n",
       " 7.451264381408691,\n",
       " 7.244190573692322,\n",
       " 7.271148204803467,\n",
       " 7.6260377168655396,\n",
       " 7.50024938583374,\n",
       " 7.638316750526428,\n",
       " 7.461502432823181,\n",
       " 7.519659876823425,\n",
       " 7.544441819190979,\n",
       " 7.77228307723999,\n",
       " 7.606389403343201,\n",
       " 7.4651899337768555,\n",
       " 7.577114939689636,\n",
       " 7.492361903190613,\n",
       " 7.548146963119507,\n",
       " 7.511637806892395,\n",
       " 7.593477964401245,\n",
       " 7.533287882804871,\n",
       " 7.6692270040512085,\n",
       " 7.601182341575623,\n",
       " 7.705509424209595,\n",
       " 7.719751358032227,\n",
       " 7.614940166473389,\n",
       " 7.654744267463684,\n",
       " 7.738580942153931,\n",
       " 7.980800151824951,\n",
       " 7.538747191429138,\n",
       " 7.560476541519165,\n",
       " 7.669429659843445,\n",
       " 7.732001423835754,\n",
       " 7.678632616996765,\n",
       " 7.979411005973816,\n",
       " 7.715738892555237,\n",
       " 7.69926130771637,\n",
       " 7.704986572265625,\n",
       " 7.72327446937561,\n",
       " 7.786914348602295,\n",
       " 7.751917004585266,\n",
       " 7.840526223182678,\n",
       " 7.962243914604187,\n",
       " 7.896237730979919,\n",
       " 8.014432191848755,\n",
       " 7.807991862297058,\n",
       " 7.864851713180542,\n",
       " 7.8910781145095825,\n",
       " 7.9421446323394775,\n",
       " 7.893756628036499,\n",
       " 8.074700355529785,\n",
       " 7.910546660423279,\n",
       " 8.060049057006836,\n",
       " 8.197112917900085,\n",
       " 7.931619763374329,\n",
       " 7.901434659957886,\n",
       " 7.96663236618042,\n",
       " 8.0972261428833,\n",
       " 7.958051919937134,\n",
       " 8.196885466575623,\n",
       " 8.127594113349915,\n",
       " 8.048149704933167,\n",
       " 8.154356837272644,\n",
       " 8.136556148529053,\n",
       " 8.282245397567749,\n",
       " 8.033483624458313,\n",
       " 8.047190546989441,\n",
       " 8.074286580085754,\n",
       " 8.08888030052185,\n",
       " 8.044500350952148,\n",
       " 8.211347818374634,\n",
       " 8.175129413604736,\n",
       " 8.130059719085693,\n",
       " 8.181066513061523,\n",
       " 8.19823145866394,\n",
       " 8.236976861953735,\n",
       " 8.176949501037598,\n",
       " 8.090849995613098,\n",
       " 8.202322125434875,\n",
       " 8.3325936794281,\n",
       " 8.181378364562988,\n",
       " 8.456299304962158,\n",
       " 8.462255716323853,\n",
       " 8.260857820510864,\n",
       " 8.39513611793518,\n",
       " 8.332295179367065,\n",
       " 8.322116017341614,\n",
       " 8.429640293121338,\n",
       " 8.542396306991577,\n",
       " 8.449851274490356,\n",
       " 8.248716115951538,\n",
       " 8.28594434261322,\n",
       " 8.23307204246521]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser wird anschließend als Grafik dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur besseren Ablesbarkeit kann die Kurve auch geglättet dargestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth_curve(points, factor=0.5):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das erste Modell erreicht bei 20 Epochen ein Minimum des mittleren absoluten Fehlers. Bei dieser Anzahl von Epochen geht das Modell von einer Unteranpassung in eine Überanpassung über. Deshalb wird das zweite Modell mit genau dieser Epochenanzahl trainiert. Damit erreicht das Modell den optimalen Lernerfolg, bei dem noch keine Überanpassung an die Trainingsdaten zu erwarten ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 2526 samples, validate on 842 samples\n",
      "Epoch 1/20\n",
      "2526/2526 [==============================] - 1s 219us/step - loss: 4026.6003 - mae: 51.2201 - val_loss: 239.8442 - val_mae: 11.7413\n",
      "Epoch 2/20\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 184.5505 - mae: 9.7780 - val_loss: 136.4685 - val_mae: 8.6596\n",
      "Epoch 3/20\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 127.1021 - mae: 7.8169 - val_loss: 96.6990 - val_mae: 7.1072\n",
      "Epoch 4/20\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 106.2179 - mae: 6.9617 - val_loss: 79.6282 - val_mae: 6.2855\n",
      "Epoch 5/20\n",
      "2526/2526 [==============================] - 0s 173us/step - loss: 96.7283 - mae: 6.4590 - val_loss: 79.2983 - val_mae: 5.6663\n",
      "Epoch 6/20\n",
      "2526/2526 [==============================] - 0s 172us/step - loss: 91.5882 - mae: 6.1917 - val_loss: 71.1938 - val_mae: 5.4319\n",
      "Epoch 7/20\n",
      "2526/2526 [==============================] - 0s 148us/step - loss: 87.7367 - mae: 6.0012 - val_loss: 72.5067 - val_mae: 6.0203\n",
      "Epoch 8/20\n",
      "2526/2526 [==============================] - 0s 164us/step - loss: 85.6615 - mae: 5.8962 - val_loss: 72.8129 - val_mae: 5.3317\n",
      "Epoch 9/20\n",
      "2526/2526 [==============================] - 0s 160us/step - loss: 84.1655 - mae: 5.8455 - val_loss: 74.2586 - val_mae: 6.2910\n",
      "Epoch 10/20\n",
      "2526/2526 [==============================] - 0s 162us/step - loss: 83.1901 - mae: 5.8321 - val_loss: 70.4481 - val_mae: 5.3853\n",
      "Epoch 11/20\n",
      "2526/2526 [==============================] - 0s 160us/step - loss: 81.8147 - mae: 5.7279 - val_loss: 69.4256 - val_mae: 5.4869\n",
      "Epoch 12/20\n",
      "2526/2526 [==============================] - 0s 155us/step - loss: 81.1756 - mae: 5.7180 - val_loss: 71.5605 - val_mae: 5.8314\n",
      "Epoch 13/20\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 81.7356 - mae: 5.7156 - val_loss: 70.6609 - val_mae: 5.4511\n",
      "Epoch 14/20\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 80.4823 - mae: 5.7393 - val_loss: 70.3231 - val_mae: 5.3885\n",
      "Epoch 15/20\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 80.8073 - mae: 5.7544 - val_loss: 72.4094 - val_mae: 5.4276\n",
      "Epoch 16/20\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 79.3973 - mae: 5.6797 - val_loss: 70.8343 - val_mae: 5.5812\n",
      "Epoch 17/20\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 78.0456 - mae: 5.6171 - val_loss: 72.5718 - val_mae: 5.6153\n",
      "Epoch 18/20\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 77.6085 - mae: 5.5941 - val_loss: 72.3758 - val_mae: 5.4188\n",
      "Epoch 19/20\n",
      "2526/2526 [==============================] - 0s 164us/step - loss: 77.0642 - mae: 5.5536 - val_loss: 71.1289 - val_mae: 5.4136\n",
      "Epoch 20/20\n",
      "2526/2526 [==============================] - 0s 166us/step - loss: 76.0681 - mae: 5.5697 - val_loss: 77.3269 - val_mae: 5.3515\n",
      "processing fold # 1\n",
      "Train on 2526 samples, validate on 842 samples\n",
      "Epoch 1/20\n",
      "2526/2526 [==============================] - 1s 208us/step - loss: 4244.6390 - mae: 53.6982 - val_loss: 309.8499 - val_mae: 12.6596\n",
      "Epoch 2/20\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 165.0437 - mae: 9.7342 - val_loss: 207.0063 - val_mae: 9.4311\n",
      "Epoch 3/20\n",
      "2526/2526 [==============================] - 0s 171us/step - loss: 108.8829 - mae: 7.6940 - val_loss: 170.5255 - val_mae: 7.9691\n",
      "Epoch 4/20\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 86.5729 - mae: 6.6407 - val_loss: 145.7574 - val_mae: 7.1755\n",
      "Epoch 5/20\n",
      "2526/2526 [==============================] - 0s 188us/step - loss: 75.9453 - mae: 6.0955 - val_loss: 137.6903 - val_mae: 6.8984\n",
      "Epoch 6/20\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 71.4886 - mae: 5.8404 - val_loss: 134.2139 - val_mae: 6.6907\n",
      "Epoch 7/20\n",
      "2526/2526 [==============================] - 0s 148us/step - loss: 68.8208 - mae: 5.6902 - val_loss: 132.5679 - val_mae: 6.4834\n",
      "Epoch 8/20\n",
      "2526/2526 [==============================] - 0s 155us/step - loss: 65.7352 - mae: 5.5406 - val_loss: 132.8424 - val_mae: 6.9799\n",
      "Epoch 9/20\n",
      "2526/2526 [==============================] - 0s 148us/step - loss: 65.5188 - mae: 5.5892 - val_loss: 134.0905 - val_mae: 6.1398\n",
      "Epoch 10/20\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 63.7973 - mae: 5.4532 - val_loss: 130.5203 - val_mae: 6.3863\n",
      "Epoch 11/20\n",
      "2526/2526 [==============================] - 0s 150us/step - loss: 63.3366 - mae: 5.4483 - val_loss: 130.7072 - val_mae: 6.7535\n",
      "Epoch 12/20\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 62.0241 - mae: 5.4489 - val_loss: 129.2615 - val_mae: 6.3082\n",
      "Epoch 13/20\n",
      "2526/2526 [==============================] - 0s 151us/step - loss: 60.6488 - mae: 5.3150 - val_loss: 131.2898 - val_mae: 6.1260\n",
      "Epoch 14/20\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 59.7378 - mae: 5.2979 - val_loss: 129.4217 - val_mae: 6.4939\n",
      "Epoch 15/20\n",
      "2526/2526 [==============================] - 0s 152us/step - loss: 59.3273 - mae: 5.2488 - val_loss: 130.6646 - val_mae: 6.2440\n",
      "Epoch 16/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 58.3009 - mae: 5.2041 - val_loss: 130.5234 - val_mae: 6.1085\n",
      "Epoch 17/20\n",
      "2526/2526 [==============================] - 0s 135us/step - loss: 57.8813 - mae: 5.2000 - val_loss: 130.9147 - val_mae: 6.2079\n",
      "Epoch 18/20\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 57.3346 - mae: 5.1421 - val_loss: 133.8323 - val_mae: 6.0276\n",
      "Epoch 19/20\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 56.8695 - mae: 5.1353 - val_loss: 129.2296 - val_mae: 6.1668\n",
      "Epoch 20/20\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 56.9292 - mae: 5.1596 - val_loss: 128.9477 - val_mae: 6.5572\n",
      "processing fold # 2\n",
      "Train on 2526 samples, validate on 842 samples\n",
      "Epoch 1/20\n",
      "2526/2526 [==============================] - 0s 196us/step - loss: 4253.4441 - mae: 53.8725 - val_loss: 197.0470 - val_mae: 10.8641\n",
      "Epoch 2/20\n",
      "2526/2526 [==============================] - 0s 146us/step - loss: 193.0182 - mae: 10.0828 - val_loss: 113.8684 - val_mae: 8.0339\n",
      "Epoch 3/20\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 121.2290 - mae: 7.5744 - val_loss: 85.0716 - val_mae: 6.8401\n",
      "Epoch 4/20\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 99.5651 - mae: 6.5536 - val_loss: 76.1041 - val_mae: 6.4084\n",
      "Epoch 5/20\n",
      "2526/2526 [==============================] - 0s 145us/step - loss: 91.7642 - mae: 6.2065 - val_loss: 72.4849 - val_mae: 5.8378\n",
      "Epoch 6/20\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 87.8243 - mae: 5.9466 - val_loss: 71.2534 - val_mae: 6.1476\n",
      "Epoch 7/20\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 85.5534 - mae: 5.9037 - val_loss: 70.6908 - val_mae: 6.2056\n",
      "Epoch 8/20\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 84.3110 - mae: 5.8284 - val_loss: 68.6268 - val_mae: 5.5678\n",
      "Epoch 9/20\n",
      "2526/2526 [==============================] - 0s 144us/step - loss: 82.8435 - mae: 5.7745 - val_loss: 67.6418 - val_mae: 5.8625\n",
      "Epoch 10/20\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 81.3065 - mae: 5.6713 - val_loss: 66.8865 - val_mae: 5.8193\n",
      "Epoch 11/20\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 79.9596 - mae: 5.6149 - val_loss: 72.3790 - val_mae: 6.5647\n",
      "Epoch 12/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 80.2657 - mae: 5.7109 - val_loss: 65.7658 - val_mae: 5.6511\n",
      "Epoch 13/20\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 78.7404 - mae: 5.6016 - val_loss: 67.1811 - val_mae: 5.9670\n",
      "Epoch 14/20\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 78.1615 - mae: 5.5674 - val_loss: 66.8477 - val_mae: 5.5069\n",
      "Epoch 15/20\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 77.9981 - mae: 5.5744 - val_loss: 65.4381 - val_mae: 5.6611\n",
      "Epoch 16/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 77.6137 - mae: 5.5686 - val_loss: 68.0844 - val_mae: 5.3793\n",
      "Epoch 17/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 76.7147 - mae: 5.5504 - val_loss: 65.2311 - val_mae: 5.4420\n",
      "Epoch 18/20\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 75.7567 - mae: 5.4621 - val_loss: 67.9096 - val_mae: 6.0168\n",
      "Epoch 19/20\n",
      "2526/2526 [==============================] - 0s 148us/step - loss: 74.9248 - mae: 5.4102 - val_loss: 66.0581 - val_mae: 5.9318\n",
      "Epoch 20/20\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 73.7363 - mae: 5.4081 - val_loss: 66.0080 - val_mae: 5.7892\n",
      "processing fold # 3\n",
      "Train on 2526 samples, validate on 842 samples\n",
      "Epoch 1/20\n",
      "2526/2526 [==============================] - 0s 194us/step - loss: 4682.5696 - mae: 57.7056 - val_loss: 259.4255 - val_mae: 12.6530\n",
      "Epoch 2/20\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 206.7063 - mae: 10.7144 - val_loss: 156.8675 - val_mae: 9.1456\n",
      "Epoch 3/20\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 134.0003 - mae: 8.2163 - val_loss: 119.7918 - val_mae: 7.5983\n",
      "Epoch 4/20\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 105.1245 - mae: 6.9253 - val_loss: 101.4854 - val_mae: 6.7572\n",
      "Epoch 5/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 93.3318 - mae: 6.3197 - val_loss: 91.6541 - val_mae: 6.2474\n",
      "Epoch 6/20\n",
      "2526/2526 [==============================] - 0s 168us/step - loss: 86.8497 - mae: 6.0067 - val_loss: 83.2440 - val_mae: 6.1822\n",
      "Epoch 7/20\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 83.1568 - mae: 5.8197 - val_loss: 81.3198 - val_mae: 6.0757\n",
      "Epoch 8/20\n",
      "2526/2526 [==============================] - 0s 140us/step - loss: 80.9989 - mae: 5.7073 - val_loss: 79.9074 - val_mae: 6.0701\n",
      "Epoch 9/20\n",
      "2526/2526 [==============================] - 0s 136us/step - loss: 79.2439 - mae: 5.5872 - val_loss: 81.2318 - val_mae: 6.5401\n",
      "Epoch 10/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 78.2468 - mae: 5.6207 - val_loss: 84.6853 - val_mae: 6.9798\n",
      "Epoch 11/20\n",
      "2526/2526 [==============================] - 0s 142us/step - loss: 77.8375 - mae: 5.5919 - val_loss: 78.3100 - val_mae: 6.1511\n",
      "Epoch 12/20\n",
      "2526/2526 [==============================] - 0s 137us/step - loss: 76.6091 - mae: 5.5246 - val_loss: 77.8243 - val_mae: 6.0362\n",
      "Epoch 13/20\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 75.1618 - mae: 5.4444 - val_loss: 79.7594 - val_mae: 5.7862\n",
      "Epoch 14/20\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 74.5498 - mae: 5.3983 - val_loss: 78.4527 - val_mae: 5.7845\n",
      "Epoch 15/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 74.0370 - mae: 5.4421 - val_loss: 79.2925 - val_mae: 5.7080\n",
      "Epoch 16/20\n",
      "2526/2526 [==============================] - 0s 141us/step - loss: 73.6208 - mae: 5.3724 - val_loss: 78.1524 - val_mae: 6.2901\n",
      "Epoch 17/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 72.8981 - mae: 5.3381 - val_loss: 78.0051 - val_mae: 6.2399\n",
      "Epoch 18/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 72.4161 - mae: 5.3359 - val_loss: 79.7497 - val_mae: 6.4918\n",
      "Epoch 19/20\n",
      "2526/2526 [==============================] - 0s 138us/step - loss: 71.5462 - mae: 5.3351 - val_loss: 77.6350 - val_mae: 6.0095\n",
      "Epoch 20/20\n",
      "2526/2526 [==============================] - 0s 139us/step - loss: 70.6439 - mae: 5.2731 - val_loss: 80.4329 - val_mae: 5.7113\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs2 = 20\n",
    "all_mae_histories2 = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_labels = train_labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = pd.concat(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_labels = pd.concat(\n",
    "        [train_labels[:i * num_val_samples],\n",
    "         train_labels[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model2 = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    history = model2.fit(partial_train_data, partial_train_labels,\n",
    "                        validation_data=(val_data, val_labels),\n",
    "                        epochs=num_epochs2, batch_size=32, verbose=1)\n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_histories2.append(mae_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Sicherheit wird dieses Modell zunächst abgespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " model2.save(r\"C:\\Users\\johan\\Desktop\\MercedesBenzGreenerManufacturing\\Model2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch für dieses Modell wird der Verlauf des Mittelwerts des mittleren absoluten Fehlers der Faltungsoperationen über den Epochen visualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_mae_history2 = [\n",
    "    np.mean([x[i] for x in all_mae_histories2]) for i in range(num_epochs2)]\n",
    "average_mae_history2\n",
    "plt.plot(range(1, len(average_mae_history2) + 1), average_mae_history2)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch diese Kurve wird zur besseren Ablesbarkeit geglättet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "smooth_mae_history = smooth_curve(average_mae_history2[:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n",
    "In der Evaluationsphase wird das optimierte Modell mit den Testdaten überprüft. \n",
    "Dazu wird zunächst mit dem optimierten Modell eine Voraussage für die Testdaten erzeugt und in das DataFrame test geschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94.589844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111.299507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.968224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.707802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93.228569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>103.756165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>100.636780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>109.284416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>91.459747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>96.500122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>841 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_pred\n",
       "0     94.589844\n",
       "1    111.299507\n",
       "2     92.968224\n",
       "3    110.707802\n",
       "4     93.228569\n",
       "..          ...\n",
       "836  103.756165\n",
       "837  100.636780\n",
       "838  109.284416\n",
       "839   91.459747\n",
       "840   96.500122\n",
       "\n",
       "[841 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.DataFrame()\n",
    "test=model2.predict(test_data)\n",
    "test=pd.DataFrame(test, columns=['y_pred'])\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weiterhin werden die Werte der tatsächlichen Prüfdauer in das Data-Frame test aufgenommen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94.589844</td>\n",
       "      <td>105.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111.299507</td>\n",
       "      <td>108.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.968224</td>\n",
       "      <td>94.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.707802</td>\n",
       "      <td>110.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93.228569</td>\n",
       "      <td>88.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>103.756165</td>\n",
       "      <td>107.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>100.636780</td>\n",
       "      <td>108.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>109.284416</td>\n",
       "      <td>109.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>91.459747</td>\n",
       "      <td>87.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>96.500122</td>\n",
       "      <td>110.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>841 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_pred  y_true\n",
       "0     94.589844  105.82\n",
       "1    111.299507  108.19\n",
       "2     92.968224   94.62\n",
       "3    110.707802  110.46\n",
       "4     93.228569   88.98\n",
       "..          ...     ...\n",
       "836  103.756165  107.39\n",
       "837  100.636780  108.77\n",
       "838  109.284416  109.22\n",
       "839   91.459747   87.48\n",
       "840   96.500122  110.85\n",
       "\n",
       "[841 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true=test_labels.reset_index().drop(columns=['index'])\n",
    "test['y_true']=y_true\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch Bildung des Absolutwertes der Differenz zwischen der vorhergesagten Prüfdauer y_pred und der tatsächlichen Prüfdauer y_true ergibt sich der absolute Vorhersagefehler absErr je Beobachtung. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "      <th>absErr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94.589844</td>\n",
       "      <td>105.82</td>\n",
       "      <td>11.230156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111.299507</td>\n",
       "      <td>108.19</td>\n",
       "      <td>3.109507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.968224</td>\n",
       "      <td>94.62</td>\n",
       "      <td>1.651776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.707802</td>\n",
       "      <td>110.46</td>\n",
       "      <td>0.247802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93.228569</td>\n",
       "      <td>88.98</td>\n",
       "      <td>4.248569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>103.756165</td>\n",
       "      <td>107.39</td>\n",
       "      <td>3.633835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>100.636780</td>\n",
       "      <td>108.77</td>\n",
       "      <td>8.133220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>109.284416</td>\n",
       "      <td>109.22</td>\n",
       "      <td>0.064416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>91.459747</td>\n",
       "      <td>87.48</td>\n",
       "      <td>3.979747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>96.500122</td>\n",
       "      <td>110.85</td>\n",
       "      <td>14.349878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>841 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_pred  y_true     absErr\n",
       "0     94.589844  105.82  11.230156\n",
       "1    111.299507  108.19   3.109507\n",
       "2     92.968224   94.62   1.651776\n",
       "3    110.707802  110.46   0.247802\n",
       "4     93.228569   88.98   4.248569\n",
       "..          ...     ...        ...\n",
       "836  103.756165  107.39   3.633835\n",
       "837  100.636780  108.77   8.133220\n",
       "838  109.284416  109.22   0.064416\n",
       "839   91.459747   87.48   3.979747\n",
       "840   96.500122  110.85  14.349878\n",
       "\n",
       "[841 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['absErr']=abs(test.y_pred-test.y_true)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Mittelwert der Spalte absErr entspricht dem Mean Absolute Error der Vorhersage über alle Beobachtungen. In diesem Fall bedeutet dies, dass die Vorhersage im Mittel einen Fehler von 5 Minuten aufweist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.000412059516317"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae=test.absErr.mean()\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fazit:</b> <br> Durch die Verwendung eines neuronalen Netzes konnte der MAE um 50% gegenüber der ursprünglichen Voraussage 'Anwendung des Mittelwerts aller Prüfwerte' reduziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y    10.088697\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend kann noch der R2-Score, das sogenannte Bestimmtheitsmaß, berechnet werden. Dieses war bei der Competition auf Kaggle.io als Bewertungskriterium für das Ranking ausschlaggebend. Dort erreichte das beste Modell einen R2-Score von 0,555. Zu beachten ist, dass im vorliegenden Beispiel ein Teil der Trainingsdaten als Testdaten verwendet wurden, weswegen das Ergebnis nicht direkt zu vergleichen ist mit den Angaben auf Kaggle.io.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6149867136956125"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2(y_true, y_pred):\n",
    "    SS_res =  np.sum(np.square(y_true - y_pred)) \n",
    "    SS_tot = np.sum(np.square(y_true - np.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot) )\n",
    "r2(test.y_true, test.y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Einsatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der erfolgreichen Evaluation des Modells kann die Implementierung in den Fachbereich erfolgen. Vorstellbar wäre z.B. eine Applikation, welche anhand der zukünftig gebauten Fahrzeuge und deren Merkmalsausprägungen die Auslastung der Prüfstände voraussagt oder die Nutzung der vorausgesagten Prüfdauer für Preiskalkulationen. So könnte dem Kunden bereits bei der Modellkonfiguration die Prüfkosten angezeigt und in Rechnung gestellt werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
